%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{multirow}

% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath

\renewcommand\refname{Referências}

\usepackage{algorithm}
\usepackage{algpseudocode}
% *** SPECIALIZED LIST PACKAGES ***
%
% \usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Detecção de Anomalias em Robôs Industriais}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{1\textsuperscript{st} Andre}
\IEEEauthorblockA{\textit{Centro de Informática - UFPE} \\
%\textit{}\\
Recife, Brazil \\
@cin.ufpe.br}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Bianca }
\IEEEauthorblockA{\textit{Centro de Informática - UFPE} \\
%\textit{name of organization (of Aff.)}\\
Recife, Brazil \\
@cin.ufpe.br}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Caio }
\IEEEauthorblockA{\textit{Centro de Informática - UFPE} \\
%\textit{name of organization (of Aff.)}\\
Recife, Brazil \\
@cin.ufpe.br}
\and
\IEEEauthorblockN{4\textsuperscript{rd} Rodrigo}
\IEEEauthorblockA{\textit{Centro de Informática - UFPE} \\
%\textit{name of organization (of Aff.)}\\
Recife, Brazil \\
@cin.ufpe.br}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Breve apresentação do contexto do trabalho, problema/tema a ser abordado, soluções existentes, método proposto e resultados obtidos.
\end{abstract}

% no keywords

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introdução}

Deverá abordar a motivação, justificativa e principais contribuições do trabalho em questão. Alguns pontos que devem ser abordados de forma \textbf{breve}, são:

\begin{itemize}
    \item \textbf{Contextualização do problema:} Qual o problema/tema que está sendo investigado?
    \item \textbf{Relevância prática do problema:} Por que ele é interessante?
\end{itemize}

\section{Análise de Dados e Feature Engineering}

\subsection{Análise Exploratória dos Dados}

\subsubsection{Análise Exploratória Estrutural}

A análise exploratória estrutural foi conduzida com o objetivo de compreender a organização e os tipos de atributos presentes no conjunto de dados. Para garantir a integridade da avaliação durante as etapas de \textit{feature selection} e modelagem de forma a prevenir a tomada de decisão enviesada e o vazamento de dados, adotou-se uma estratégia de particionamento dos dados a priori. A divisão em subconjuntos de Treino, Validação e Teste foi realizada imediatamente após a coleta. O Conjunto de Validação, composto por 10\% dos dados Normais e 50\% dos dados de Falha (de cada cenário), foi utilizado para a Análise Exploratória de Dados (EDA) e demais etapas subsequentes.

O dataset é composto por séries temporais de sensores inerciais, contendo variáveis numéricas contínuas associadas às medições de acelerômetro, giroscópio e magnetômetro, nos eixos X, Y e Z; uma coluna temporal que representa o instante de coleta de cada amostra e uma variável indicativa da condição de operação do robô, utilizada como rótulo para diferenciar entre comportamento normal e anômalo. Particularmente, os dados anômalos são separados de acordo com o tipo de falha, sendo elas:

\begin{itemize}
  \item Hitting Platform, que lida com colisão contra a plataforma;
  \item Hitting Arm, colisão contra o próprio braço robótico;
  \item Extra Weight, esforço mecânico dado por peso extra;
  \item Earthquake, vibração estrutural externa.
\end{itemize}

Cada observação corresponde a uma leitura dos sensores em um determinado instante de tempo, sendo os dados organizados de forma sequencial. A variável temporal é representada por valores inteiros de alta resolução, indicando registros em escala de nanosegundos, reforçando o caráter temporal do problema. No entanto, com o objetivo de facilitar a interpretação e a análise de integridade temporal, os valores foram convertidos para milissegundos, mantendo a proporcionalidade entre as amostras e reduzindo a magnitude numérica dos registros. 

Durante a análise estrutural, foi identificada a presença de uma coluna textual associada à identificação do sensor, que por apresentar valor constante em todas as observações e não agregar informação discriminativa ao problema de detecção de anomalias, essa variável foi removida do conjunto de dados. Adicionalmente, foi realizada a verificação de duplicidade nos registros temporais, não sendo identificadas amostras com timestamps repetidos, o que indica consistência na indexação temporal das observações.

\subsubsection{Informações básicas}
O conjunto de dados normais analisado é composto por 874.937 amostras e 11 atributos após a etapa inicial de pré-processamento. Todas as variáveis de sensores são numéricas contínuas do tipo \textit{float64}, enquanto a variável alvo é representada por valores inteiros binários. O volume total de memória ocupado por esse dataset é de aproximadamente 73 MB.

Por outro lado, o conjunto de dados anômalos é composto por 49.185 amostras com os 11 atributos contínuos numéricos e a variável alvo binária. Com um volume total de memória de aproximadamente 10MB. Adicionalmente, o dataset possui um atributo \textit{scenario} que indica o tipo de falha a qual esse registro é associado, seguindo a proporção destacada na tabela abaixo.

\begin{table}[h]
\centering
\caption{Quantidade de anomalias por tipo de falha}
\label{tab:quantidade-anomalia-tipo-falha}
\begin{tabular}{llcc}
\hline
\multicolumn{1}{c}{\textbf{Cenário}} & \textbf{Quantidade} & \textbf{Proporção} \\ \hline
Hitting Platform & 14.967 & 0.3043 \\
Hitting Arm & 11.924 & 0.2424 \\
Earthquake & 11.409 & 0.2320 \\
Extra Weight & 10.885 & 0.2213
\end{tabular}
\end{table}

\subsubsection{Análise de Duplicatas, Valores Faltantes e \textit{Outliers}}
A integridade dos dados foi avaliada sob três perspectivas: redundância de registros, continuidade temporal e presença de valores extremos.

\paragraph{Duplicatas e Valores Nulos Explícitos}
Uma varredura inicial no conjunto de dados brutos não identificou linhas duplicadas ou valores nulos (\textit{NaN}) explícitos nas leituras dos sensores, tanto nos dados normais quanto anômalos. O sistema de aquisição registrou continuamente as 9 variáveis do IMU (acelerômetros, giroscópios e magnetômetros) sem falhas de escrita evidentes.

\paragraph{Análise de Continuidade Temporal}
Embora não houvesse \textit{NaNs} no arquivo original, a análise do intervalo entre amostras revelou um problema crítico de integridade temporal. A frequência nominal de coleta, era de 10 Hz (com \textit{Sampling Rate} de 100 ms). No entanto, observou-se um \textit{Jitter} (desvio padrão do intervalo de tempo) relevante de aproximadamente 22 ms, com 429 \textit{Gaps} Temporais significativos variando entre 2 ms e 342 ms, onde essa quantidade de perda de pacotes foi considerada quando o valor do \textit{Gap} foi maior que 2x a média, sendo categorizada como quebra de continuidade.

Para padronizar a frequência e facilitar o janelamento das séries temporais nas etapas seguintes, foi utilizada a técnica de \textit{Resampling}, impondo uma grade temporal rígida de 100 ms. Nesse contexto, a irregularidade original foi exposta sob a forma de \textit{Gaps} temporais. No conjunto de dados Normal, o alinhamento gerou cerca de 44.766 lacunas onde não havia dados registrados no timestamp esperado.

Para corrigir os problemas mencionados anteriormente sem descartar dados, optou-se pela \textbf{interpolação linear} dos valores faltantes gerados pelo \textit{Resampling}. Essa abordagem preservou a tendência do movimento entre os pontos conhecidos, restaurando a continuidade necessária para a extração de \textit{features} de janela deslizante.

Para os conjuntos de dados anômalos, a etapa de regularização temporal com \textit{Resampling} e Interpolação foi aplicada de forma iterativa e independente para cada cenário de falha em detrimento a aplicação direta sobre o conjunto anômalo completo. Essa abordagem foi necessária para prevenir as descontinuidades temporais, visto que os experimentos de falha foram registrados em contextos diferentes e com \textit{Gaps} de minutos a horas entre si e a imposição de uma grade temporal contínua de 10Hz no dataset completo forçaria a criação de milhares de linhas vazias para preencher esses intervalos e a interpolação linear geraria dados sintéticos falsos, o que tornaria inviável alimentar o modelo com esses dados, mesmo com etapas subsequentes de pré-processamento.

\paragraph{Análise de \textit{Outliers}}
A análise de distribuição, por meior de Boxplots e Histogramas, detectou uma quantidade massiva de \textit{outliers} estatísticos, especialmente nos eixos do acelerômetro. Nos dados de colisão (\textit{Hitting Arm}), a curtose do eixo Z atingiu valores extremos ($>100$), com picos de aceleração de até 10g, muito superiores à faixa média normal de operação ($\pm 1g$). Entretenato, diferente de problemas de regressão clássicos, onde \textit{outliers} são ruídos a serem removidos, neste projeto eles constituem o próprio \textbf{sinal de interesse} (a falha mecânica). Consequentemente, optou-se por \textbf{não remover} os \textit{outliers}. 
Para mitigar o impacto desses valores extremos, na normalização dos dados mais a frente, substituiu-se o \textit{StandardScaler} que é um \textit{scaler} sensível à média e desvio padrão, pelo \textbf{\textit{RobustScaler}}, pois ele utiliza a mediana e o intervalo interquartil (IQR), garantindo que os picos de colisão permaneçam destacados na escala transformada, preservando a assinatura da anomalia para o modelo. A normalização foi feita com o \textit{fit} dos dados normais \textit{resampled} e transformação tanto dos dados normais quanto dos anômalos \textit{resampled} com a concatenação dos cenários de falha.

\subsubsection{Análise Univariada} \label{subsec:analise-univariada}
Executar análises entre atributos e alvo de forma univariada com descrição estatística e visualizações de apoio.

\subsubsection{Análise Bivariada}

A análise bivariada partiu da hipótese de que a ocorrência de anomalias afeta o estado físico do robô, questionou-se se as falhas seriam capazes de "quebrar" o acoplamento mecânico entre sensores redundantes ou se as alterações seriam mais perceptíveis em pares de baixa correlação. Para investigar essa premissa, foram definidos cenários analíticos estratégicos, cujo objetivo foi garantir que diferentes tipos de relação entre sensores fossem explicitamente investigados. Essa escolha é importante porque pares de variáveis com comportamentos distintos (alta correlação, correlação média ou baixa correlação) tendem a responder de maneira diferente à introdução de falhas, fornecendo evidências complementares sobre a dinâmica do sistema. A métrica utilizada para quantificar a divergência entre as distribuições do alvo normal e anômalo foi a distância de Jensen-Shannon.

Foram definidos seis cenários bivariados. Nos dois cenários de redundância (pares com maior correlação física), a importância reside em avaliar sensores fortemente acoplados. Nesses casos, observou-se que, mesmo na presença de falhas, o padrão conjunto se mantém relativamente estável. Quantitativamente, isso se traduziu nas menores distâncias estatísticas observadas: o par (accx,accy) apresentou uma distância JS de apenas 0.12, seguido pelo par (accy,gyroy) com 0.14. Esses baixos valores indicam que falhas nem sempre rompem relações altamente correlacionadas, sugerindo que pares muito redundantes carregam informação similar e tendem a variar de forma conjunta, mesmo em condições anômalas.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{imagens/redundancia.png}
\end{figure}

O cenário de linha de base, definido por um par com correlação próxima à média do sistema, foi incluído para representar um comportamento típico. O par (accx,magx) com uma distância JS de 0.20, ilustra essa referência. Esse valor intermediário permite avaliar como a resposta observada em cenários extremos de fato se diferencia do comportamento esperado em uma interação padrão entre sensores.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{imagens/media.png}
\end{figure}

Já no cenário de complementaridade, composto por sensores pouco correlacionados entre si, observou-se a mudança mais pronunciada entre os dados normais e de falha. A baixa correlação implica que cada sensor responde a aspectos diferentes do fenômeno físico. Quando ocorre uma falha, essas respostas divergentes ampliam drasticamente a separação entre as distribuições conjuntas. Isso ficou evidente nos pares: (accz,magz) atingiu a maior distância registrada (0.61). Esse alto valor confirma que a combinação de variáveis complementares é a mais sensível para capturar a divergência estatística introduzida pelas anomalias.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{imagens/diferentes.png}
\end{figure}

Por fim, os dois cenários de poder preditivo concentraram-se nos sensores mais correlacionados com o rótulo de falha. A importância desses cenários está em avaliar se sensores individualmente informativos também apresentam mudanças relevantes quando analisados em conjunto. Os resultados indicaram que a introdução de falhas altera a densidade conjunta, embora de forma menos extrema que na complementaridade, ou seja, o que importa não é unicamente a importância do sensor, mas também sua correlação para definir se o par vai se comportar de maneira mais parecida ou não.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{imagens/top2.png}
\end{figure}

Em suma, a análise validou a hipótese inicial: pares altamente correlacionados tendem a mascarar a anomalia, enquanto pares com baixa correlação evidenciam as alterações de forma mais clara.

\subsubsection{Análise Multivariada}

Na análise multivariada, todas as variáveis dos sensores foram consideradas simultaneamente por meio do PCA (Principal Component Analysis), após padronização. A projeção nos dois primeiros componentes revelou que as anomalias não são facilmente separáveis dos dados normais. As amostras de falha aparecem cercadas por pontos normais e distribuídas em diferentes regiões do espaço reduzido, indicando que não existe um único padrão geométrico simples que caracterize todas as falhas.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{imagens/top2.png}
\end{figure}

Apesar dessa sobreposição, a visualização evidenciou diferenças estruturais relevantes. Em diversas regiões do espaço PCA, observam-se concentrações de falhas com orientações e dispersões distintas, refletindo o impacto específico de cada tipo de anomalia sobre o sistema. Isso indica que, embora não haja separação linear clara, as falhas alteram a dinâmica multivariada de maneira consistente, produzindo padrões locais diferenciados.

As setas do biplot permitiram interpretar a contribuição de cada sensor para essas diferenças, destacando quais variáveis influenciam mais fortemente determinadas direções do espaço PCA. Sensores com vetores mais extensos tiveram maior peso na organização dos dados, ajudando a explicar por que determinadas anomalias se manifestam em regiões específicas do plano.

A análise foi complementada pela visualização temporal de janelas de 30 segundos no espaço PCA, utilizando uma régua visual fixa para todos os cenários. Essa abordagem evidenciou que o comportamento normal tende a produzir trajetórias mais compactas e regulares, enquanto os cenários de falha apresentam trajetórias mais dispersas e irregulares, reforçando o impacto dinâmico das anomalias ao longo do tempo.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{imagens/temporal.png}
\end{figure}

No que se refere às variáveis derivadas, foram calculadas magnitudes físicas da aceleração, do giroscópio e do magnetômetro, com o objetivo de condensar informações multieixo em medidas escalares fisicamente interpretáveis. 

Testes exploratórios básicos, como a comparação de distribuições entre classes e análises de separação estatística, indicaram que essas variáveis capturam alterações relevantes introduzidas pelas falhas, mostrando potencial para uso na otimização dos modelos subsequentes, permitindo, por exemplo, estratégias de redução de dimensionalidade que mantenham a interpretabilidade física do fenômeno ao diminuir o número de variáveis de entrada.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{imagens/importancia.png}
\end{figure}

Por fim, aplicou-se o K-Means para identificar grupos de interesse no espaço formado pelas variáveis originais. O uso de um método não supervisionado é adequado nesse contexto, pois permite investigar a estrutura intrínseca dos dados sem recorrer aos rótulos. Os resultados mostraram que, considerando apenas os dados brutos, não foi possível identificar um grupo claramente dominante em termos de concentração de falhas.

\begin{table}[H]
    \centering
    \begin{tabular}{c c c}
        \hline
        \textbf{Cluster} & \textbf{Normal (\%)} & \textbf{Falha (\%)} \\
        \hline
        0 & 96.601872 & 3.398128 \\
        1 & 96.528435 & 3.471565 \\
        2 & 96.646539 & 3.353461 \\
        \hline
    \end{tabular}
\end{table}


\subsection{Pré-processamento dos dados}
Apresentar ações referentes ao pré-processamento de dados a exemplo de:

\subsubsection{Tratamento de Valores Faltantes}
Em caso de valores faltantes, descrever o tratamento executado para sanar os problemas com justificativas bem fundamentadas.

\subsubsection{Tratamento de Outliers}
Em caso de \textit{outliers}, apresentar o tratamento executado com devidas justificativas.

\subsubsection{Detecção e Tratamento de Duplicadas}
Avaliar se há presença de duplicadas no dataset utilizado e tratá-las de acordo.

\subsubsection{Feature Scaling}
Utilizar ferramentas de normalização como StandardScaler ou MinMaxScaler para normalizar os atributos.

\subsubsection{Encoding de Variáveis Categóricas}
Utilizar ferramentas adequadas para tratamento de variáveis. Indicar qual técnica foi utilizada e a motivação.

\subsection{Divisão dos Dados}
Justificar e apresentar a divisão de dados entre Treino, Validação e Teste com justificativas a respeito da técnica utilizada.

\subsection{Feature Engineering}
Deverá abordar a seleção e extração de \textit{features} caso cabível. Em caso de utilização de técnicas de redução de dimensionalidade, justificar a escolha do método e configurações.

\section{Modelagem}
Três modelos foram escolhidos para esse projeto: Gaussian Mixture Model (GMM), Isolation Forest e Autoencoder Convolucional. Todos os modelos condizem com o problema proposto: são próprios para aprendizado não-supervisionado, cada um possuindo uma abordagem distinta para tanto: clustering, isolamento e aprendizado profundo baseado em redes neurais, respectivamente, tornando-os apropriados para comparar a eficácia desses diferentes métodos para o dataset em questão. O processo de seleção de modelos utilizou como métrica principal a AUC. No entanto, devido ao uso de predições binárias em vez de escores contínuos, essa métrica não corresponde estritamente à AUROC tradicional, funcionando como uma medida de desempenho baseada em classificação após a aplicação de um limiar
\subsection{Gaussian Mixture Model}
\subsubsection{Conceitos Básicos} O modelo de mistura gaussiana (gaussian mixture model, GMM) é um modelo probabilístico que assume que os dados são descritos por uma mistura de distribuições gaussianas.

Formalmente, um GMM modela a densidade de probabilidade de uma observação
$\mathbf{x} \in \mathbb{R}^d$ como uma soma ponderada de $K$ componentes Gaussianos,
dada por:

\begin{equation}
p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k),
\end{equation}

onde $\pi_k$ representa o peso da $k$-ésima componente, satisfazendo
$\sum_{k=1}^{K} \pi_k = 1$, e $\boldsymbol{\mu}_k$ e $\boldsymbol{\Sigma}_k$ correspondem,
respectivamente, ao vetor de médias e à matriz de covariância da componente $k$. Os parâmetros são encontrados por meio da maximização de expectativa, alternando entre momento de estimação das probabilidades e atualização dos parâmetros.

O GMM é um algoritmo de soft clustering, ou seja, atribui probabilidades ao invés de classificar unicamente a um dos componentes, tornando-o mais robusto. Serão classificadas como anomalias os dados que possuam baixa probabilidade (log-verossimilhança).
\subsubsection{Justificativa} O GMM foi escolhido primeiramente por ser um modelo de clustering, uma das abordagens mais importantes para aprendizagem não-supervisionada. Entre os algoritmos desse tipo, o GMM se destaca por utilizar o soft clustering probabilistico e modelar os grupos com base em multiplas distribuições, lidando bem com dados mais complexos. 

Por fim, o GMM apresenta uma boa relação entre expressividade e custo computacional, sendo capaz de modelar correlações entre variáveis (por meio de matrizes de covariância completas) sem exigir grandes volumes de dados ou recursos computacionais elevados. Dessa forma, o GMM se configura como uma escolha sólida e complementar aos modelos de isolamento e aprendizado profundo considerados neste trabalho, contribuindo para uma comparação abrangente entre diferentes paradigmas de detecção de anomalias aplicados ao dataset em questão.
\subsubsection{Espaço de Busca} Para a tunagem foi utilizado o grid search. Os parâmetros buscados para descobrir o melhor número de componentes foram limitados a números baixos devido à natureza do problema: o braço robótico, que deve seguir movimentos repetitivos em condições normais, não precisará de muitos clusters, como foi verificado pelo seu melhor parâmetro.
\begin{table}[ht]
\centering
\caption{Espaço de busca dos hiperparâmetros utilizados na tunagem do modelo Gaussian Mixture Model (GMM).}
\label{tab:gmm_search_space}
\begin{tabular}{ll}
\hline
\textbf{Categoria} & \textbf{Hiperparâmetro e valores considerados} \\
\hline
\multirow{2}{*}{Janelamento} 
& (\texttt{new\_window\_size}): \{30, 60\} \\
& (\texttt{window\_overlap}): \{0, 15\} \\
\hline
\multirow{4}{*}{GMM}
& (\texttt{n\_components}): \{1, 2, 3, 4\} \\
& (\texttt{covariance\_type}): \{\texttt{diag}, \texttt{full}\} \\
& (\texttt{reg\_covar}): \{1e{-}5\} \\
& (\texttt{threshold\_percentile}): \{1, 2.5, 5\} \\
\hline
\end{tabular}
\end{table}
\subsubsection{Hiperparâmetros Selecionados} Para a janela, o melhor foi com tamanho 60 e overlap 0. Para o modelo, foi 1 componente, tipo de covariância 'diag' e limiar 1.
\subsection{Isolation Forest}
\subsubsection{Conceitos Básicos} O Isolation Forest é um modelo de aprendizagem não-supervisionada bastante relacionado com detecção de anomalias. Diferentes de outros modelos que também são frequentemente usados para essa área, o Isolation Forest utiliza o princípio de isolamento, isto é, dado que anomalias devem ser bem diferenciáveis dos dados normais, elas serão mais fáceis de isolar com um particionamento aleatório.

O algoritmo constrói um conjunto de árvores binárias, denominadas isolation trees, nas quais cada nó interno realiza uma divisão aleatória escolhendo um atributo e um valor de corte também aleatório dentro do intervalo observado desse atributo. O processo continua recursivamente até que a instância seja isolada em um nó folha ou até que seja atingida uma profundidade máxima.

Para definir se um dado é anômalo ou não, o modelo utiliza o comprimento médio do caminho, ou seja, a profundida média necessária para isolá-lo. O anomaly score gerado parte do princípio que dados anômalos serão isolados mais cedos que os demais.
\subsubsection{Justificativa} Primeiramente, esse modelo foi projetado precisamente para problemas como o nosso: não-supervisionado, com detecção de anomalias raras, e utiliza uma abordagem diferente da dos outros modelos escolhidos. 

Além disso, ele funciona como um bom contraponto para o clustering previamente testado: datasets como o que usamos, relacionado à sensores industriais, costumam ter muitas dimensões, especialmente com o janelamento temporal, o que impacta negativamente modelos como K-Means e GMM, a chamada "maldição da dimensionalidade". Por não depender de métricas de distância globais, o Isolation Forest evita esse problema. O cárater aleatório e de ensemble dele também auxilia na detecção de anomalias de vários tipos, como é o caso desse dataset.
\subsubsection{Espaço de Busca} a tunagem foi feita utilizando o tradicional método de grid search, testando todas as combinações dos parâmetros possíveis detalhados na tabela. 
\begin{table}[ht]
\centering
\caption{Espaço de busca dos hiperparâmetros utilizados na tunagem do modelo Isolation Forest.}
\label{tab:isoforest_search_space}
\begin{tabular}{ll}
\hline
\textbf{Categoria} & \textbf{Hiperparâmetro e valores considerados} \\
\hline
\multirow{2}{*}{Janelamento} 
&(\texttt{new\_window\_size}): \{30, 60\} \\
& (\texttt{window\_overlap}): \{0, 15\} \\
\hline
\multirow{3}{*}{Isolation Forest}
& (\texttt{n\_estimators}): \{50, 100, 150, 200, 250, 300\} \\
&(\texttt{max\_samples}): \{\texttt{auto}, 0.25, 0.5, 0.75\} \\
& (\texttt{contamination}): \{\texttt{auto}, 0.01, 0.05, 0.1\} \\
\hline
\end{tabular}
\end{table}
\subsubsection{Hiperparâmetros Selecionados} para a janela, o tamanho e o overlap foram 60 e 15, respectivamente. No modelo, os melhores parâmetros foram 50 àrvores, 0.75 fração de amostras por árvore e taxa de contaminação automática ('auto').
\subsection{Autoencoder Convolucional}
\subsubsection{Conceitos Básicos} 
Autoencoders são modelos de aprendizado não supervisionado baseados em redes neurais artificiais cujo objetivo é aprender uma representação compacta dos dados por meio da reconstrução da entrada. Formalmente, um autoencoder é composto por duas funções principais: um codificador, que projeta os dados de entrada em um espaço latente de menor dimensionalidade, e um decodificador, responsável por reconstruir os dados originais a partir dessa representação comprimida. O treinamento é realizado minimizando uma função de perda de reconstrução, tipicamente o erro quadrático médio (MSE), entre a entrada e sua reconstrução.

No caso de Autoencoders Convolucionais 1D, camadas densas são substituídas por camadas convolucionais unidimensionais, que exploram explicitamente a estrutura temporal ou sequencial dos dados. As convoluções 1D aplicam filtros locais ao longo do eixo temporal, permitindo ao modelo capturar padrões de curto alcance, como tendências locais, transições abruptas e correlações temporais entre variáveis sensoriais, auxiliado por operações como pooling e upsampling, permitindo reduzir e recuperar as representações.

Para detecção de anomalias, o autoencoder é treinado exclusivamente com dados considerados normais, aprendendo a reconstruir adequadamente padrões recorrentes do comportamento normal, enquanto apresenta maior erro de reconstrução para amostras divergentes. Assim, o erro de reconstrução passa a ser utilizado como uma métrica contínua de anomalia, permitindo a definição de um limiar para classificação.
\subsubsection{Justificativa}
A escolha de um Autoencoder Convolucional 1D para este projeto se justifica principalmente pela natureza temporal e multivariada dos dados analisados. Diferentemente de métodos baseados apenas em distribuição estatística ou isolamento geométrico, autoencoders convolucionais são capazes de aprender relações não lineares complexas entre múltiplos sensores ao longo do tempo, sem a necessidade de rótulos explícitos. Além disso, o uso de convoluções 1D confere ao modelo invariância local e capacidade de generalização, algo muito valioso em ambientes que podem ter ruído.

É um modelo bastante robusto, baseado em aprendizagem profunda, capaz de modelar relações complexas e altamente não-lineares.  

Por fim, a abordagem baseada em erro de reconstrução permite uma separação clara entre dados normais e anômalos, como evidenciado pelos resultados experimentais obtidos neste trabalho, nos quais as anomalias apresentaram erros de reconstrução significativamente superiores aos observados em dados normais. Essa característica torna o Autoencoder Convolucional uma ferramenta eficaz e robusta para detecção de anomalias em séries temporais multivariadas.
\subsubsection{Espaço de Busca} Foi feito um grid search para tunagem de hiperparâmetros. Antes da tunagem propriamente dita, diversas arquiteturas foram analisadas manualmente para delimitar nosso espaço de busca, evitando uma grid search muito extensa e demorada, testando configurações mais complexas e com mais camadas, chegando até a 64 filtros. No entanto, elas apresentaram resultados semelhantes a arquiteturas mais simples de cerca de 8 filtros. Para evitar overfitting e custo computacional elevado, optamos pelos parâmetros abaixo.
\begin{table}[ht]
\centering
\caption{Espaço de busca dos hiperparâmetros utilizados na tunagem do Autoencoder Convolucional 1D.}
\label{tab:ae_search_space}
\begin{tabular}{ll}
\hline
\textbf{Categoria} & \textbf{Hiperparâmetro e valores considerados} \\
\hline
\multirow{5}{*}{Autoencoder Convolucional 1D}
& (\texttt{filters}): \{\{8,8\}, \{8,6\}, \{12,8\}\} \\
& (\texttt{kernel\_sizes}): \{\{3,3\}, \{5,3\}\} \\
& (\texttt{latent\_channels}): \{4, 8\} \\
& (\texttt{batch\_size}): \{64\} \\
& (\texttt{epochs}): \{60\} \\
& (\texttt{lr}): \{$10^{-3}$\} \\
& (\texttt{patience}): \{5\} \\
& (\texttt{percentile}): \{95.0, 97.5, 99\} \\
\hline
\end{tabular}
\end{table}

\subsubsection{Hiperparâmetros Selecionados}

\section{Análise e Comparação de Resultados}
Deverá conter as métricas que foram utilizadas para a análise juntamente com revisão de conceito e justificativas. É fundamental comparar os resultados obtidos entre os diferentes modelos treinados. Além disso, é interessante utilizar ferramentas estatísticas e/ou testes de hipótese quando cabível.
\begin{table}[ht]
\centering
\caption{Resultados de desempenho do modelo Gaussian Mixture Model (GMM) no conjunto de teste.}
\label{tab:gmm_results}
\begin{tabular}{|l|c|}
\hline
\textbf{Métrica} & \textbf{Valor} \\
\hline
Acurácia (Accuracy) & 0.8256 \\
Precisão (Anomalia como positiva) & 0.1899 \\
Revocação (Recall – Anomalia) & 1.0000 \\
F1-score (Anomalia) & 0.3192 \\
ROC AUC & 1.0000 \\
PR AUC (Average Precision) & 1.0000 \\
Média de log-verossimilhança (normal) & -1672.9433 \\
Média de log-verossimilhança (anômalo) & -12554031.9700 \\
\hline
\end{tabular}
\end{table}
\begin{table}[ht]
\centering
\caption{Matriz de confusão do modelo Gaussian Mixture Model (GMM) no conjunto de teste.}
\label{tab:gmm_confusion}
\begin{tabular}{|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Classe Real}} & \multicolumn{2}{c|}{\textbf{Classe Predita}} \\
\cline{2-3}
 & \textbf{Anomalia} & \textbf{Normal} \\
\hline
\textbf{Anomalia} & 166 & 0 \\
\hline
\textbf{Normal} & 708 & 3186 \\
\hline
\end{tabular}
\end{table}

O Gaussian Mixture Model (GMM) alcançou recall igual a 1,0, detectando todas as anomalias presentes no conjunto de teste, o que evidencia elevada sensibilidade. No entanto, essa capacidade foi acompanhada por um número expressivo de falsos positivos, refletido em uma precisão de aproximadamente 0,19 e em um F1-score de cerca de 0,32, ainda que superior ao obtido pelo Isolation Forest. Apesar desse desempenho binário moderado, as métricas baseadas em ranqueamento revelam um cenário significativamente mais favorável: tanto a ROC AUC quanto a PR AUC atingiram o valor máximo (1,0). Essa discrepância indica que o GMM foi capaz de separar perfeitamente amostras normais e anômalas no espaço de escores contínuos (log-verossimilhança), mas que o limiar adotado para a decisão final não foi ótimo. Tal interpretação é corroborada pela diferença extrema observada entre as log-verossimilhanças médias das amostras normais e anômalas, sugerindo que as anomalias apresentam padrões estatísticos profundamente incompatíveis com a distribuição aprendida a partir dos dados normais.

A escolha do número de componentes igual a um para o melhor GMM reforça essa interpretação. Um modelo unimodal assume que os dados normais podem ser adequadamente descritos por uma única distribuição Gaussiana, tratando desvios significativos como anômalos. Nesse contexto, a excelente separação observada nos scores sugere que o conjunto de dados normal é relativamente homogêneo, enquanto as anomalias representam desvios extremos, o que favorece abordagens baseadas em densidade. Ainda assim, a taxa de falsos positivos indica a necessidade de uma estratégia de calibração de limiar mais criteriosa, por exemplo, baseada na maximização do F1-score ou em restrições de precisão mínima no conjunto de validação.

\begin{table}[ht]
\centering
\caption{Resultados de desempenho do modelo Isolation Forest no conjunto de teste.}
\label{tab:isoforest_results}
\begin{tabular}{|l|c|}
\hline
\textbf{Métrica} & \textbf{Valor} \\
\hline
Acurácia (Accuracy) & 0.8579 \\
Precisão (Anomalia como positiva) & 0.1487 \\
Recall – Anomalia & 0.5241 \\
F1-score (Anomalia) & 0.2317 \\
ROC AUC & 0.7745 \\
PR AUC (Average Precision) & 0.3212 \\
\hline
\end{tabular}
\end{table}
\begin{table}[ht]
\centering
\caption{Matriz de confusão do modelo Isolation Forest no conjunto de teste.}
\label{tab:isoforest_confusion}
\begin{tabular}{|c|c|c|}
\hline
\multirow{2}{*}{\textbf{Classe Real}} & \multicolumn{2}{c|}{\textbf{Classe Predita}} \\
\cline{2-3}
 & \textbf{Anomalia} & \textbf{Normal} \\
\hline
\textbf{Anomalia} & 87 & 79 \\
\hline
\textbf{Normal} & 498 & 3396 \\
\hline
\end{tabular}
\end{table}

O Isolation Forest apresentou acurácia global de aproximadamente 85,8\%, valor influenciado principalmente pelo forte desbalanceamento entre amostras normais e anômalas. Apesar dessa acurácia relativamente elevada, o modelo obteve baixa precisão para a classe anômala (cerca de 0,15), indicando que a maioria das amostras sinalizadas como anômalas corresponde, na realidade, a padrões normais. Em contrapartida, o recall de aproximadamente 0,52 sugere que o modelo foi capaz de identificar pouco mais da metade das anomalias reais presentes no conjunto de teste. Esse comportamento resulta em um F1-score reduzido (cerca de 0,23), evidenciando um compromisso desfavorável entre sensibilidade e precisão. A métrica ROC AUC de 0,77 indica que, embora o modelo possua capacidade razoável de ranqueamento entre padrões normais e anômalos, sua fronteira de decisão não é suficientemente discriminativa para fornecer decisões binárias confiáveis sem uma calibração adicional do limiar de decisão. Somado à isso, o isolation forest foi o único modelo que classificou algumas anomalias como normais, algo que seria bastante problemático no ambiente de produção.

\section{Conclusão e Discussão}
Explicar os principais achados ao longo do trabalho bem como vantagens e limitações de métodos e/ou algoritmos selecionados. Além disso, apresentar principais \textit{insights} extraídos e potenciais trabalhos futuros.

\section*{Material de Apoio}
% ##################### DICA ########################
% Tabelas em latex são ruins de serem manipuladas, para isso, existem ferramentas que ajudam a gerar o "código" da tabela com ajuda de uma interface gráfica
% Exemplo: https://www.tablesgenerator.com/
% Exemplo: https://tableconvert.com/csv-to-latex
\begin{table}[h]
\centering
\caption{Exemplo de tabela}
\label{tab:tabela-exemplo}
\begin{tabular}{llcc}
\hline
\multicolumn{1}{c}{\textbf{Trabalhos}} & \textbf{Método} & \textbf{Vantagens} & \textbf{Desvantagens} \\ \hline
Trabalho A & Método X & A & D \\
Trabalho B & Método Y & G & J
\end{tabular}
\end{table}
\section{Modelo de ameaça (SE APLICÁVEL)}

Utilizar figuras e/ou algoritmos e/ou equações para descrever os comportamentos da análise. 

% Utilize o comando "\eqref{}" para se referenciar a equações
Na Eq. \eqref{equacao-da-reta} é apresentado um exemplo de equação com a equação de uma reta.

\begin{equation}
    y = ax + b
    \label{equacao-da-reta}
\end{equation}

Na Fig. \ref{fig: logo-cin} é apresentado um exemplo de figura com a logo do centro de informática.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{imagens/logo_cin_ufpe.png}
        \caption{Logo do centro de informática.}
        \label{fig: logo-cin}
\end{figure}

No Alg. \ref{alg:cap} é apresentado um exemplo de algoritmo.

\begin{algorithm}
\caption{Algoritmo com legenda}\label{alg:cap}
\begin{algorithmic}
\Require $n \geq 0$
\Ensure $y = x^n$
\State $y \gets 1$
\State $X \gets x$
\State $N \gets n$
\While{$N \neq 0$}
\If{$N$ is even}
    \State $X \gets X \times X$
    \State $N \gets \frac{N}{2}$  \Comment{Exemplo de comentário}
\ElsIf{$N$ is odd}
    \State $y \gets y \times X$
    \State $N \gets N - 1$
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].

% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\begin{thebibliography}{1}
\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}




% that's all folks
\end{document}