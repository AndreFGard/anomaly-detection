{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCvMw0sfXpdV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBFiu9QRX5Na"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import ks_2samp\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    \"\"\"Classe para gerenciar coleta e carregamento de dados de IMU de braço robótico\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_name='hkayan/industrial-robotic-arm-imu-data-casper-1-and-2'):\n",
        "        \"\"\"\n",
        "        Inicializa o Dataset\n",
        "\n",
        "        Args:\n",
        "            dataset_name: Nome do dataset no Kaggle\n",
        "        \"\"\"\n",
        "        self.dataset_name = dataset_name\n",
        "        self.dataset_path = self._obter_caminho_dataset()\n",
        "        self.df_normal = None\n",
        "        self.df_faulty = None\n",
        "        self.lista_dfs_anomaly = None\n",
        "        self.df_combined = None\n",
        "        self._carregar_dados()\n",
        "        self._combinar_dados()\n",
        "\n",
        "    def _obter_caminho_dataset(self):\n",
        "        \"\"\"Obtém o caminho do dataset (local ou download)\"\"\"\n",
        "        caminho = os.environ.get(\"DATASET_PATH\")\n",
        "        if not caminho:\n",
        "            caminho = kagglehub.dataset_download(self.dataset_name) + '/'\n",
        "        return caminho\n",
        "\n",
        "    def listar_arquivos(self, caminho='/kaggle/input'):\n",
        "        \"\"\"Lista todos os arquivos disponíveis no diretório\"\"\"\n",
        "        arquivos = []\n",
        "        for dirname, _, filenames in os.walk(caminho):\n",
        "            for filename in filenames:\n",
        "                arquivo_completo = os.path.join(dirname, filename)\n",
        "                print(arquivo_completo)\n",
        "                arquivos.append(arquivo_completo)\n",
        "        return arquivos\n",
        "\n",
        "    def _carregar_dados(self, arquivo_normal='IMU_10Hz.csv'):\n",
        "        \"\"\"\n",
        "        Carrega os dados normais e todos os tipos de falha disponíveis\n",
        "\n",
        "        Args:\n",
        "            arquivo_normal: Nome do arquivo com dados normais\n",
        "\n",
        "        Returns:\n",
        "            tuple: (df_normal, df_faulty, lista_dfs_anomaly)\n",
        "        \"\"\"\n",
        "        print(\"--- CARREGAMENTO MANUAL DE CENÁRIOS ---\")\n",
        "\n",
        "        # 1. Carregar o NORMAL (df)\n",
        "        # ------------------------------------------------------------------\n",
        "        print(\"Lendo Base Normal...\")\n",
        "        self.df_normal = pd.read_csv(self.dataset_path + arquivo_normal)\n",
        "        self.df_normal['label'] = 0\n",
        "        self.df_normal['scenario'] = 'Normal'\n",
        "\n",
        "        # 2. Carregar AS ANOMALIAS (faultydf)\n",
        "        # ------------------------------------------------------------------\n",
        "        print(\"Lendo Base de Falhas...\")\n",
        "\n",
        "        # Lista simples e direta com TODOS os arquivos de problema disponíveis\n",
        "        arquivos_falha = [\n",
        "            'IMU_hitting_platform.csv',   # Colisão: Plataforma\n",
        "            'IMU_hitting_arm.csv',        # Colisão: Braço (Robô se batendo)\n",
        "            'IMU_extra_weigth.csv',       # Mecânico: Peso Extra (Esforço)\n",
        "            'IMU_earthquake.csv',         # Ambiental: Terremoto (Vibração externa)\n",
        "        ]\n",
        "\n",
        "        lista_dfs = []\n",
        "\n",
        "        for arquivo in arquivos_falha:\n",
        "            # Carrega cada um individualmente\n",
        "            temp_df = pd.read_csv(self.dataset_path + arquivo)\n",
        "\n",
        "            # Padroniza\n",
        "            temp_df['label'] = 1             # Todo mundo aqui é erro\n",
        "            temp_df['scenario'] = arquivo    # Guarda o nome pra você saber o que é\n",
        "\n",
        "            lista_dfs.append(temp_df)\n",
        "            print(f\"-> Adicionado: {arquivo} ({len(temp_df)} linhas)\")\n",
        "\n",
        "        self.lista_dfs_anomaly = lista_dfs\n",
        "\n",
        "        df_train, df_val, df_test = self.split_train_val_test()\n",
        "\n",
        "        # Junta todos os arquivos da lista em um só DataFrame\n",
        "        self.df_faulty = df_val[df_val['label'] == 1].copy()\n",
        "        self.lista_dfs_anomaly = [df for scenario, df in self.df_faulty.groupby('scenario')]\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(f\"DATASET PRONTO:\")\n",
        "        print(f\"-> Dados Normais: {len(self.df_normal)} linhas\")\n",
        "        print(f\"-> Dados de Falha:  {len(self.df_faulty)} linhas (Total de 4 tipos de defeito)\")\n",
        "\n",
        "        return self.df_normal, self.df_faulty, self.lista_dfs_anomaly\n",
        "\n",
        "    def _combinar_dados(self):\n",
        "        \"\"\"\n",
        "        Combina os datasets normal e faulty em um único DataFrame\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Dataset combinado\n",
        "        \"\"\"\n",
        "        if self.df_normal is None or self.df_faulty is None:\n",
        "            raise ValueError(\"Carregue os dados primeiro usando _carregar_dados()\")\n",
        "\n",
        "        self.df_combined = pd.concat([self.df_normal, self.df_faulty],\n",
        "                                      ignore_index=True)\n",
        "        return self.df_combined\n",
        "\n",
        "    def obter_info(self):\n",
        "        \"\"\"Retorna informações sobre os datasets carregados\"\"\"\n",
        "        info = {}\n",
        "        if self.df_normal is not None:\n",
        "            info['normal'] = {\n",
        "                'shape': self.df_normal.shape,\n",
        "                'colunas': list(self.df_normal.columns)\n",
        "            }\n",
        "        if self.df_faulty is not None:\n",
        "            info['faulty'] = {\n",
        "                'shape': self.df_faulty.shape,\n",
        "                'colunas': list(self.df_faulty.columns)\n",
        "            }\n",
        "        if self.df_combined is not None:\n",
        "            info['combined'] = {\n",
        "                'shape': self.df_combined.shape,\n",
        "                'distribuicao_labels': self.df_combined['label'].value_counts().to_dict()\n",
        "            }\n",
        "        return info\n",
        "\n",
        "    @staticmethod\n",
        "    def split_sequencial(df, p_train=0.7, p_val=0.1, p_test=0.2):\n",
        "        \"\"\"Corta um DataFrame em 3 pedaços sequenciais baseados nas porcentagens.\"\"\"\n",
        "        size = len(df)\n",
        "        end_train = int(size * p_train)\n",
        "        end_val = int(size * (p_train + p_val))\n",
        "\n",
        "        train = df.iloc[:end_train].copy()\n",
        "        val = df.iloc[end_train:end_val].copy()\n",
        "        test = df.iloc[end_val:].copy()\n",
        "\n",
        "        return train, val, test\n",
        "\n",
        "    def split_train_val_test(self):\n",
        "        norm_train, norm_val, norm_test = self.split_sequencial(self.df_normal)\n",
        "        # Listas para acumular os pedaços (começamos com o normal)\n",
        "        final_train_list = [norm_train]\n",
        "        final_val_list   = [norm_val]\n",
        "        final_test_list  = [norm_test]\n",
        "\n",
        "        print(f\"1. Normal processado: {len(self.df_normal)} linhas divididas.\")\n",
        "\n",
        "        for df_falhas in self.lista_dfs_anomaly:\n",
        "            f_train, f_val, f_test = self.split_sequencial(df_falhas,0,0.5,0.5)\n",
        "            print(f\"Falha {df_falhas.iloc[0]['scenario']} processado\" )\n",
        "            final_val_list.append(f_val)\n",
        "            final_test_list.append(f_test)\n",
        "\n",
        "        df_train_final = pd.concat(final_train_list, ignore_index=True)\n",
        "        df_val_final = pd.concat(final_val_list, ignore_index=True)\n",
        "        df_test_final = pd.concat(final_test_list, ignore_index=True)\n",
        "\n",
        "        return df_train_final, df_val_final, df_test_final\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CimtSlxdSNV6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import ks_2samp\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "\n",
        "class EDA:\n",
        "    \"\"\"Classe para análise exploratória de dados de IMU de braço robótico\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_name='hkayan/industrial-robotic-arm-imu-data-casper-1-and-2'):\n",
        "        \"\"\"\n",
        "        Inicializa a classe EDA com um dataset\n",
        "\n",
        "        Args:\n",
        "            dataset_name: Nome do dataset no Kaggle\n",
        "        \"\"\"\n",
        "        self.dataset = Dataset(dataset_name)\n",
        "        self.df_normal = None\n",
        "        self.df_faulty = None\n",
        "        self.df_normal_resampled = None\n",
        "        self.df_faulty_resampled = None\n",
        "        self.df_normal_robust_scaled = None\n",
        "        self.df_faulty_robust_scaled = None\n",
        "        self.df_normal_standard_scaled = None\n",
        "        self.df_faulty_standard_scaled = None\n",
        "        self.scaler_robust = None\n",
        "        self.scaler_standard = None\n",
        "        self.lista_dfs_anomaly = []\n",
        "        self.dict_scenarios = {}\n",
        "        self.faultydfs_resampled = {}\n",
        "\n",
        "    def carregar_e_preparar_dados(self, arquivo_normal='IMU_10Hz.csv'):\n",
        "        \"\"\"\n",
        "        Carrega e prepara os dados para análise.\n",
        "        Carrega dados normais e todos os 4 tipos de anomalias disponíveis.\n",
        "\n",
        "        Args:\n",
        "            arquivo_normal: Nome do arquivo com dados normais\n",
        "\n",
        "        Returns:\n",
        "            tuple: (df_normal, df_faulty)\n",
        "        \"\"\"\n",
        "        # Os dados já foram carregados no __init__ do Dataset\n",
        "        # Apenas referencia os DataFrames já existentes\n",
        "        self.df_normal = self.dataset.df_normal\n",
        "        self.df_faulty = self.dataset.df_faulty # não será usado agora, ele será o concat do resampling de cada df_anomaly\n",
        "        self.lista_dfs_anomaly = self.dataset.lista_dfs_anomaly\n",
        "        self.dict_scenarios = {df['scenario'].iloc[0] : df for df in self.lista_dfs_anomaly}\n",
        "\n",
        "        # Conversões e limpezas iniciais\n",
        "        print(\"\\nConvertendo `time` de nanossegundos para milissegundos\")\n",
        "        self.df_normal['time'] = self.df_normal['time'].map(lambda x: x/1e6)\n",
        "        self.df_faulty['time'] = self.df_faulty['time'].map(lambda x: x/1e6)\n",
        "\n",
        "        print(\"\\nRemovendo a coluna `name` e `scenario` em df_normal pois não agrega valor preditivo\")\n",
        "        cols_to_drop = ['name', 'scenario']\n",
        "        self.df_normal = self.df_normal.drop(columns=cols_to_drop)\n",
        "\n",
        "        for scenario_name in self.dict_scenarios:\n",
        "            self.dict_scenarios[scenario_name] = self.dict_scenarios[scenario_name].drop(columns=cols_to_drop)\n",
        "            self.dict_scenarios[scenario_name]['time'] = (self.dict_scenarios[scenario_name]['time'].map(lambda x: x/1e6))\n",
        "\n",
        "        print(\"\\nIdentificando duplicatas\")\n",
        "        print(f\"[Normal] Duplicatas de Tempo: {self.df_normal['time'].duplicated().any()}\")\n",
        "        for scenario_name in self.dict_scenarios:\n",
        "            print(f\"[{scenario_name}] duplicatas de tempo: {self.dict_scenarios[scenario_name]['time'].duplicated().any()}\")\n",
        "\n",
        "        return self.df_normal, self.df_faulty\n",
        "\n",
        "    def executar_analise_completa(self):\n",
        "        \"\"\"Executa a análise exploratória completa\"\"\"\n",
        "        if self.df_normal is None or self.dict_scenarios is {}:\n",
        "            raise ValueError(\"Carregue os dados primeiro usando carregar_e_preparar_dados()\")\n",
        "\n",
        "        print(\"\"\"## Análise Exploratória Estrutural [Classe Normal]\n",
        "        - Informações básicas do dataset\n",
        "        - Tipos de dados\n",
        "        - Informações detalhadas\n",
        "        - Estatísticas descritivas\n",
        "        - Análise de valores únicos\"\"\")\n",
        "\n",
        "        # Análise estrutural\n",
        "        self.analise_estrutural_sensores(self.df_normal)\n",
        "\n",
        "        # Visualizações\n",
        "        sensores = ['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ', 'magX', 'magY', 'magZ']\n",
        "        print(\"\\n[NORMAL] Visualizando os Acelerômetros:\")\n",
        "        for sensor in sensores[0:3]:\n",
        "            self.plot_sensor(self.df_normal, col_sensor=sensor)\n",
        "        print(\"\\n[NORMAL] Visualizando os Giroscópios:\")\n",
        "        for sensor in sensores[3:6]:\n",
        "            self.plot_sensor(self.df_normal, col_sensor=sensor)\n",
        "        print(\"\\n[NORMAL] Visualizando os Magnetômetros:\")\n",
        "        for sensor in sensores[6:9]:\n",
        "            self.plot_sensor(self.df_normal, col_sensor=sensor)\n",
        "\n",
        "        print(\"\"\"## Análise Exploratória Estrutural [Classe Anômala]\n",
        "        - Informações básicas do dataset\n",
        "        - Tipos de dados\n",
        "        - Informações detalhadas\n",
        "        - Estatísticas descritivas\n",
        "        - Análise de valores únicos\"\"\")\n",
        "\n",
        "        print(self.df_faulty['scenario'].value_counts())\n",
        "\n",
        "        print(\"\\nAnalisando por tipo de anomalia\")\n",
        "        for scenario_name in self.dict_scenarios:\n",
        "            print(f\"\\n-------Análise Estrutural dos Sensores em {scenario_name}-------\")\n",
        "            self.analise_estrutural_sensores(self.dict_scenarios[scenario_name])\n",
        "\n",
        "        print(\"\\nVisualizando `accZ` em diferentes anomalias\")\n",
        "        for scenario_name in self.dict_scenarios:\n",
        "            print(f\"\\n-----------{scenario_name}-----------\")\n",
        "            self.plot_sensor(self.dict_scenarios[scenario_name], col_sensor='accZ')\n",
        "\n",
        "        # Comparação Normal vs Falha\n",
        "        self.comparar_normal_vs_falha(self.df_normal, self.dict_scenarios['IMU_hitting_arm.csv'], col_sensor='accZ', anomalia='IMU_hitting_arm.csv')\n",
        "        self.comparar_normal_vs_falha(self.df_normal, self.dict_scenarios['IMU_earthquake.csv'], col_sensor='accZ', anomalia='IMU_earthquake.csv')\n",
        "        self.comparar_normal_vs_falha(self.df_normal, self.dict_scenarios['IMU_hitting_arm.csv'], col_sensor='gyroY', anomalia='IMU_hitting_arm.csv')\n",
        "        self.comparar_normal_vs_falha(self.df_normal, self.dict_scenarios['IMU_extra_weigth.csv'], col_sensor='accX', anomalia='IMU_extra_weigth.csv')\n",
        "        self.comparar_normal_vs_falha(self.df_normal, self.dict_scenarios['IMU_extra_weigth.csv'], col_sensor='accY', anomalia='IMU_extra_weigth.csv')\n",
        "\n",
        "        # Análise de valores faltantes\n",
        "        print(\"\"\"\\n## Análise de Valores Faltantes e Outliers\n",
        "        - Identificação de valores faltantes e outliers\n",
        "        - Visualizações de apoio, caso necessário\n",
        "        - Análise dos mecanismos\\n\"\"\")\n",
        "\n",
        "        print(f\"Quantidade de valores faltantes em df_normal: {self.df_normal.isnull().sum().sum()}\")\n",
        "        for scenario_name in self.dict_scenarios:\n",
        "            print(f\"[{scenario_name}] quantidade de Valores faltantes: {self.dict_scenarios[scenario_name].isnull().sum().sum()}\")\n",
        "        print(f\"\\nNão serão removidos os outliers estatísticos pois são movimentos reais\")\n",
        "\n",
        "    def analise_estrutural_sensores(self, df, time_col='time', label_col='label'):\n",
        "        \"\"\"\n",
        "        Realiza uma análise exploratória estrutural focada em dados de sensores (Séries Temporais).\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame com os dados.\n",
        "            time_col: Nome da coluna de tempo.\n",
        "            label_col: Nome da coluna de target (anomalia/normal).\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(\"1. INFORMAÇÕES BÁSICAS E TIPOS DE DADOS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Dimensões do Dataset: {df.shape[0]} linhas, {df.shape[1]} colunas\")\n",
        "        print(\"\\nTipos de Dados (Dtypes):\")\n",
        "        print(df.dtypes)\n",
        "\n",
        "        # Verificação de memória\n",
        "        memoria = df.memory_usage(deep=True).sum() / 1024**2\n",
        "        print(f\"\\nUso de Memória: {memoria:.2f} MB\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"2. ANÁLISE DE INTEGRIDADE TEMPORAL (CRÍTICO PARA IMU)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Ordenar por tempo para garantir análise correta\n",
        "        df = df.sort_values(by=time_col)\n",
        "\n",
        "        # Calcular diferenças de tempo (Delta T)\n",
        "        delta_t = df[time_col].diff().dropna()\n",
        "\n",
        "        mean_dt = delta_t.mean()\n",
        "        std_dt = delta_t.std()\n",
        "        min_dt = delta_t.min()\n",
        "        max_dt = delta_t.max()\n",
        "\n",
        "        print(f\"- Intervalo de Amostragem Médio (Sampling Rate): {mean_dt:.6f} ms\")\n",
        "        print(f\"- Frequência de coleta de dados: {1/(df['time'].diff().mean()/1000):.2f} Hz\")\n",
        "        print(f\"- Jitter (Desvio Padrão do tempo): {std_dt:.6f} ms\")\n",
        "        print(f\"- Gap Mínimo: {min_dt:.6f} ms | Gap Máximo: {max_dt:.6f} ms\")\n",
        "\n",
        "        # Verificar se há gaps significativos (perda de pacotes)\n",
        "        # Exemplo: Se o gap for maior que 2x a média, é uma quebra de continuidade\n",
        "        gaps = (delta_t > 2 * mean_dt).sum()\n",
        "        print(f\"- Qtd. de Gaps Temporais Significativos (> 2x média): {gaps}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"3. ANÁLISE DE VALORES ÚNICOS E CONSTANTES (SENSOR FREEZE)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Separa colunas de sensores (excluindo tempo e label)\n",
        "        cols_sensores = [c for c in df.columns if c not in [time_col, label_col]]\n",
        "\n",
        "        resumo_unicos = pd.DataFrame({\n",
        "            'Tipo': df[cols_sensores].dtypes,\n",
        "            'Qtd_Unicos': df[cols_sensores].nunique(),\n",
        "            'Unicos (%)': (df[cols_sensores].nunique() / len(df)) * 100,\n",
        "            'Qtd_Zeros': (df[cols_sensores] == 0).sum(),\n",
        "            'Zeros (%)': ((df[cols_sensores] == 0).sum() / len(df)) * 100\n",
        "        })\n",
        "\n",
        "        print(resumo_unicos.sort_values('Qtd_Unicos'))\n",
        "\n",
        "        # Alerta para colunas com baixíssima variabilidade (Sensor travado ou irrelevante)\n",
        "        cols_travadas = resumo_unicos[resumo_unicos['Qtd_Unicos'] == 1].index.tolist()\n",
        "        if cols_travadas:\n",
        "            print(f\"\\n[ALERTA] Colunas com valor constante (irrelevantes): {cols_travadas}\")\n",
        "        else:\n",
        "            print(\"\\n[OK] Nenhuma coluna totalmente constante detectada.\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"4. ESTATÍSTICAS DESCRITIVAS DETALHADAS (MOMENTOS)\")\n",
        "        print(\"=\"*60)\n",
        "        # Inclui Skewness e Kurtosis que são vitais para detectar desvios de normalidade em sinais\n",
        "        desc = df[cols_sensores].describe().T\n",
        "        desc['skewness'] = df[cols_sensores].skew()\n",
        "        desc['kurtosis'] = df[cols_sensores].kurt()\n",
        "\n",
        "        print(desc[['mean', 'std', 'min', '50%', 'max', 'skewness', 'kurtosis']])\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"5. BALANCEAMENTO DAS CLASSES (TARGET)\")\n",
        "        print(\"=\"*60)\n",
        "        if label_col in df.columns:\n",
        "            contagem = df[label_col].value_counts()\n",
        "            percentual = df[label_col].value_counts(normalize=True) * 100\n",
        "\n",
        "            balanceamento = pd.DataFrame({'Contagem': contagem, 'Percentual (%)': percentual})\n",
        "            print(balanceamento)\n",
        "\n",
        "            ratio = contagem.max() / contagem.min() if len(contagem) > 1 else 0\n",
        "            print(f\"\\nRazão de Desbalanceamento: 1 : {ratio:.1f}\")\n",
        "        else:\n",
        "            print(f\"Coluna de target '{label_col}' não encontrada.\")\n",
        "\n",
        "    def plot_sensor(self, df, col_sensor, col_time='time'):\n",
        "        \"\"\"\n",
        "        Gera um painel triplo para diagnosticar o comportamento do sensor.\n",
        "        1. Série Temporal (Visão Geral)\n",
        "        2. Histograma (Verificar Curtose e Zeros)\n",
        "        3. Boxplot (Verificar Outliers Extremos)\n",
        "        \"\"\"\n",
        "\n",
        "        # Copia para não alterar o original\n",
        "        df_plot = df.copy()\n",
        "\n",
        "        # Converter tempo para segundos para ficar legível no eixo X\n",
        "        df_plot['time_sec'] = (df_plot[col_time] - df_plot[col_time].iloc[0]) / 1e3\n",
        "\n",
        "        # Configuração da Figura\n",
        "        fig = plt.figure(figsize=(18, 10))\n",
        "        gs = fig.add_gridspec(2, 2)\n",
        "\n",
        "        # --- PLOT 1: SÉRIE TEMPORAL ---\n",
        "        ax1 = fig.add_subplot(gs[0, :])\n",
        "        ax1.plot(df_plot['time_sec'], df_plot[col_sensor], color='#1f77b4', linewidth=0.5, alpha=0.8)\n",
        "        ax1.set_title(f'Série Temporal: {col_sensor}', fontsize=14, fontweight='bold')\n",
        "        ax1.set_xlabel('Tempo (segundos)', fontsize=12)\n",
        "        ax1.set_ylabel('Valor do Sensor', fontsize=12)\n",
        "        ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "        # Destacar a linha do Zero (Ociosidade)\n",
        "        ax1.axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.7, label='Zero (Repouso)')\n",
        "        ax1.legend()\n",
        "\n",
        "        # --- PLOT 2: DISTRIBUIÇÃO (HISTOGRAMA + KDE) ---\n",
        "        ax2 = fig.add_subplot(gs[1, 0])\n",
        "        # Usamos escala logarítmica no Y devido a quantidade de zeros\n",
        "        sns.histplot(data=df_plot, x=col_sensor, bins=100, kde=True, ax=ax2, color='#2ca02c')\n",
        "        ax2.set_yscale('log') # Escala Log para ver as caudas pequenas e o pico gigante\n",
        "        ax2.set_title(f'Distribuição (Escala Log): {col_sensor}', fontsize=14, fontweight='bold')\n",
        "        ax2.set_xlabel('Valor do Sensor')\n",
        "        ax2.set_ylabel('Frequência (Log)')\n",
        "\n",
        "        # Anotação da Curtose\n",
        "        kurt = df_plot[col_sensor].kurt()\n",
        "        ax2.text(0.95, 0.95, f'Kurtosis: {kurt:.2f}', transform=ax2.transAxes,\n",
        "                horizontalalignment='right', verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
        "\n",
        "        # --- PLOT 3: BOXPLOT (DETECÇÃO DE OUTLIERS) ---\n",
        "        ax3 = fig.add_subplot(gs[1, 1])\n",
        "        sns.boxplot(x=df_plot[col_sensor], ax=ax3, color='#ff7f0e', fliersize=3)\n",
        "        ax3.set_title(f'Boxplot de Outliers: {col_sensor}', fontsize=14, fontweight='bold')\n",
        "        ax3.set_xlabel('Valor do Sensor')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def comparar_normal_vs_falha(self, df_normal, df_falha, col_sensor, anomalia):\n",
        "        \"\"\"\n",
        "        Plota comparativo visual entre operação Normal e Falha (Ataque/Colisão).\n",
        "        \"\"\"\n",
        "        # 1. Ajuste de Tempo (reseta para começar do zero em ambos para facilitar visualização)\n",
        "        t_norm = (df_normal['time'] - df_normal['time'].iloc[0]) / 1e3\n",
        "        t_fail = (df_falha['time'] - df_falha['time'].iloc[0]) / 1e3\n",
        "\n",
        "        # Recorte: Pegar apenas os primeiros 10 segundos de cada para não poluir\n",
        "        mask_norm = t_norm <= 10\n",
        "        mask_fail = t_fail <= 10\n",
        "\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
        "\n",
        "        # --- PLOT 1: COMPARAÇÃO NO TEMPO ---\n",
        "        # Normal\n",
        "        axes[0].plot(t_norm[mask_norm], df_normal.loc[mask_norm, col_sensor],\n",
        "                    color='#1f77b4', label='Normal', alpha=0.7, linewidth=1)\n",
        "        axes[0].set_title(f'Padrão Normal vs. Falha - {anomalia} ({col_sensor})', fontsize=14, fontweight='bold')\n",
        "        axes[0].set_ylabel('Valor do Sensor')\n",
        "        axes[0].set_xlabel('Tempo [s]')\n",
        "        axes[0].legend(loc='upper left')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Falha (mesmo eixo para ver a diferença de magnitude)\n",
        "        axes[0].plot(t_fail[mask_fail], df_falha.loc[mask_fail, col_sensor],\n",
        "                    color='#d62728', label='Falha', alpha=0.7, linewidth=1)\n",
        "        axes[0].legend()\n",
        "\n",
        "        # --- PLOT 2: COMPARAÇÃO DE DENSIDADE (KDE) ---\n",
        "        # Mostra se a \"forma\" dos dados mudou\n",
        "        sns.kdeplot(df_normal[col_sensor], ax=axes[1], color='#1f77b4', fill=True, label='Normal')\n",
        "        sns.kdeplot(df_falha[col_sensor], ax=axes[1], color='#d62728', fill=True, label='Falha')\n",
        "        axes[1].set_title(f'Mudança na Distribuição de Probabilidade - {anomalia}', fontsize=14, fontweight='bold')\n",
        "        axes[1].set_yscale('log') # Log para ver as caudas\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def resampling_and_interpolate(self, df, df_name):\n",
        "        \"\"\"\n",
        "        Faz resampling e interpolação dos dados para frequência fixa de 100ms\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame com os dados\n",
        "            df_name: Nome do dataset (para logging)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame reamostrado e interpolado\n",
        "        \"\"\"\n",
        "        print(f\"\\nFazendo Resampling e Interpolate de {df_name}\")\n",
        "        # 1. Converter tempo para Datetime (necessário para resampling)\n",
        "        df = df.copy()\n",
        "        df['datetime'] = pd.to_datetime(df['time'], unit='ms')\n",
        "        df = df.set_index('datetime')\n",
        "\n",
        "        # 2. Resampling para 100ms (10Hz) - Ajuste conforme a média que vimos\n",
        "        # .mean() pega todos os pontos que caíram naquele 0.1s e tira a média (reduz ruído)\n",
        "        df_resampled = df.resample('100ms').mean()\n",
        "\n",
        "        # 3. Verificar onde ficaram os buracos (NaNs gerados pelo resampling)\n",
        "        print(f\"Buracos gerados pelo alinhamento: {df_resampled['accX'].isnull().sum()}\")\n",
        "\n",
        "        # 4. Preencher buracos com Interpolação Linear\n",
        "        # 'time' garante que a interpolação respeite a distância temporal\n",
        "        df_final = df_resampled.interpolate(method='time')\n",
        "\n",
        "        # 5. Drop nas colunas que não fazem sentido interpolar (ex: label)\n",
        "        if 'label' in df_final.columns:\n",
        "            df_final['label'] = df_resampled['label'].ffill().astype(int)\n",
        "\n",
        "        print(\"Resampling concluído. Novo shape:\", df_final.shape)\n",
        "\n",
        "        return df_final\n",
        "\n",
        "    def aplicar_resampling(self):\n",
        "        \"\"\"Aplica resampling nos datasets normal e faulty\"\"\"\n",
        "        if self.df_normal is None or self.df_faulty is None:\n",
        "            raise ValueError(\"Carregue os dados primeiro usando carregar_e_preparar_dados()\")\n",
        "\n",
        "        self.df_normal_resampled = self.resampling_and_interpolate(self.df_normal, \"df_normal\")\n",
        "        for name, df in self.dict_scenarios.items():\n",
        "            self.faultydfs_resampled[name] = self.resampling_and_interpolate(df, name)\n",
        "\n",
        "        self.df_faulty_resampled = pd.concat(self.faultydfs_resampled.values(), ignore_index=True)\n",
        "\n",
        "        return self.df_normal_resampled, self.df_faulty_resampled\n",
        "\n",
        "    def _normalizar_df(self,df,fit_scaler=True,scaler=None):\n",
        "        \"\"\"Normaliza e retorna apenas um df\"\"\"\n",
        "        if fit_scaler:\n",
        "            scaler = RobustScaler()\n",
        "            scaler.fit(df)\n",
        "            df_scaled = scaler.transform(df)\n",
        "        else:\n",
        "            df_scaled = scaler.transform(df) #type:ignore\n",
        "        return df_scaled,scaler\n",
        "\n",
        "    def aplicar_normalizacao(self):\n",
        "        \"\"\"Aplica normalização RobustScaler e StandardScaler nos dados reamostrados\"\"\"\n",
        "        if self.df_normal_resampled is None or self.df_faulty_resampled is None:\n",
        "            raise ValueError(\"Execute o resampling primeiro usando aplicar_resampling()\")\n",
        "\n",
        "        # RobustScaler\n",
        "        self.scaler_robust = RobustScaler()\n",
        "        self.scaler_robust.fit(self.df_normal_resampled)\n",
        "\n",
        "        self.df_normal_robust_scaled = self.scaler_robust.transform(self.df_normal_resampled)\n",
        "        self.df_faulty_robust_scaled = self.scaler_robust.transform(self.df_faulty_resampled)\n",
        "\n",
        "        # StandardScaler\n",
        "        self.scaler_standard = StandardScaler()\n",
        "        self.scaler_standard.fit(self.df_normal_resampled)\n",
        "\n",
        "        self.df_normal_standard_scaled = self.scaler_standard.transform(self.df_normal_resampled)\n",
        "        self.df_faulty_standard_scaled = self.scaler_standard.transform(self.df_faulty_resampled)\n",
        "\n",
        "        print(\"\\nNormalização concluída com RobustScaler e StandardScaler\")\n",
        "\n",
        "        return (self.df_normal_robust_scaled, self.df_faulty_robust_scaled,\n",
        "                self.df_normal_standard_scaled, self.df_faulty_standard_scaled)\n",
        "\n",
        "    def diagnostico_preprocessamento(self, df_raw, df_resampled, df_scaled,\n",
        "                                     col_sensor='accZ',\n",
        "                                     window_sec=1.0,\n",
        "                                     scaler_name='RobustScaler'):\n",
        "        \"\"\"\n",
        "        Gera um relatório visual comparando os estágios de pré-processamento.\n",
        "\n",
        "        Args:\n",
        "            df_raw: DataFrame original (Bruto).\n",
        "            df_resampled: DataFrame após resampling e interpolação.\n",
        "            df_scaled: DataFrame (ou array) após aplicação do Scaler.\n",
        "            col_sensor: Nome da coluna do sensor para focar a análise.\n",
        "            window_sec: Janela de tempo (em segundos) para o zoom do resampling.\n",
        "            scaler_name: Nome do scaler usado (apenas para título).\n",
        "        \"\"\"\n",
        "\n",
        "        # Configurar a figura\n",
        "        fig = plt.figure(figsize=(18, 12))\n",
        "        gs = fig.add_gridspec(2, 2, height_ratios=[1, 1])\n",
        "\n",
        "        # ====================================================================\n",
        "        # PARTE 1: EFEITO DO RESAMPLING (ZOOM NO TEMPO)\n",
        "        # ====================================================================\n",
        "        ax1 = fig.add_subplot(gs[0, :])\n",
        "\n",
        "        # Preparar dados de tempo para plotagem\n",
        "        # Assumindo que df_raw['time'] é int (ns) e df_resampled index é datetime\n",
        "        if 'time' in df_raw.columns:\n",
        "            t_raw = (df_raw['time'] - df_raw['time'].iloc[0]) / 1e3\n",
        "        else:\n",
        "            # Tenta usar o índice se não tiver coluna time\n",
        "            t_raw = np.arange(len(df_raw))\n",
        "\n",
        "        t_res = (df_resampled.index - df_resampled.index[0]).total_seconds()\n",
        "\n",
        "        # Recorte (Zoom) para ver os detalhes\n",
        "        # Pegamos apenas os primeiros 'window_sec' segundos\n",
        "        mask_raw = t_raw <= window_sec\n",
        "        mask_res = t_res <= window_sec\n",
        "\n",
        "        # Plotar pontos originais (Scatter para mostrar o Jitter/Irregularidade)\n",
        "        ax1.scatter(t_raw[mask_raw], df_raw.loc[mask_raw, col_sensor],\n",
        "                    color='black', alpha=0.6, s=30, label='Original (Raw Points)', zorder=3)\n",
        "\n",
        "        # Plotar linha reamostrada (Linha + X para mostrar a grade fixa)\n",
        "        ax1.plot(t_res[mask_res], df_resampled.loc[mask_res, col_sensor],\n",
        "                 color='#1f77b4', linewidth=2, marker='x', markersize=8,\n",
        "                 label='Resampled (10Hz Grid)', alpha=0.8, zorder=2)\n",
        "\n",
        "        ax1.set_title(f'1. Efeito do Resampling: Regularização do Tempo ({col_sensor}) - Zoom de {window_sec}s',\n",
        "                      fontsize=14, fontweight='bold')\n",
        "        ax1.set_xlabel('Tempo (segundos)')\n",
        "        ax1.set_ylabel('Valor do Sensor (Físico)')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "        # ====================================================================\n",
        "        # PARTE 2: EFEITO DO SCALER (DISTRIBUIÇÃO)\n",
        "        # ====================================================================\n",
        "\n",
        "        # Preparar o df_scaled se ele for um numpy array (saída comum do sklearn)\n",
        "        if isinstance(df_scaled, np.ndarray):\n",
        "            # Tenta encontrar o índice da coluna se for array\n",
        "            try:\n",
        "                col_idx = df_resampled.columns.get_loc(col_sensor)\n",
        "                data_scaled = df_scaled[:, col_idx]\n",
        "            except:\n",
        "                data_scaled = df_scaled[:, 0] # Fallback\n",
        "        else:\n",
        "            data_scaled = df_scaled[col_sensor]\n",
        "\n",
        "        # Plot A: Distribuição Original (Resampled)\n",
        "        ax2 = fig.add_subplot(gs[1, 0])\n",
        "        sns.histplot(df_resampled[col_sensor], kde=True, ax=ax2, color='#1f77b4', bins=50)\n",
        "        ax2.set_title(f'2a. Distribuição ANTES do Scaler\\n(Unidades Físicas Reais)', fontsize=12, fontweight='bold')\n",
        "        ax2.set_xlabel(f'{col_sensor} Original')\n",
        "\n",
        "        # Plot B: Distribuição Escalada\n",
        "        ax3 = fig.add_subplot(gs[1, 1])\n",
        "        sns.histplot(data_scaled, kde=True, ax=ax3, color='#2ca02c', bins=50)\n",
        "        ax3.set_title(f'2b. Distribuição DEPOIS do {scaler_name}\\n(Unidades Relativas)', fontsize=12, fontweight='bold')\n",
        "        ax3.set_xlabel(f'{col_sensor} Scaled')\n",
        "\n",
        "        # Adicionar estatísticas de texto para comparação\n",
        "        orig_mean, orig_std = df_resampled[col_sensor].mean(), df_resampled[col_sensor].std()\n",
        "        scale_mean, scale_std = np.mean(data_scaled), np.std(data_scaled)\n",
        "\n",
        "        txt = (f\"Original:\\nMédia={orig_mean:.2f}\\nStd={orig_std:.2f}\\nMin={df_resampled[col_sensor].min():.2f}\\nMax={df_resampled[col_sensor].max():.2f}\")\n",
        "        ax2.text(0.95, 0.95, txt, transform=ax2.transAxes, ha='right', va='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        txt_sc = (f\"Scaled:\\nMédia={scale_mean:.2f}\\nStd={scale_std:.2f}\\nMin={np.min(data_scaled):.2f}\\nMax={np.max(data_scaled):.2f}\")\n",
        "        ax3.text(0.95, 0.95, txt_sc, transform=ax3.transAxes, ha='right', va='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def analise_univariada_alvo(self, df_normal=None, df_falha=None, anomalia=''):\n",
        "        \"\"\"\n",
        "        Realiza a análise univariada comparando atributos vs. alvo (Normal vs Falha).\n",
        "        Gera estatísticas de separação e visualizações.\n",
        "\n",
        "        Args:\n",
        "            df_normal: DataFrame com dados normais (usa self.df_normal_resampled se None)\n",
        "            df_falha: DataFrame com dados de falha (usa self.df_faulty_resampled se None)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame com estatísticas de ranking dos sensores\n",
        "        \"\"\"\n",
        "        if df_normal is None:\n",
        "            df_normal = self.df_normal_resampled\n",
        "        if df_falha is None:\n",
        "            df_falha = self.df_faulty_resampled\n",
        "\n",
        "        if df_normal is None or df_falha is None:\n",
        "            raise ValueError(\"Execute o resampling primeiro ou forneça os DataFrames\")\n",
        "\n",
        "        # 1. Preparar dados\n",
        "        # Remover colunas não-sensor (time, label, datetime, name)\n",
        "        cols_ignore = ['time', 'label', 'datetime', 'name']\n",
        "        sensores = [c for c in df_normal.columns if c not in cols_ignore]\n",
        "\n",
        "        stats_list = []\n",
        "\n",
        "        # Configuração dos Plots\n",
        "        # Vamos fazer um grid de 3 colunas\n",
        "        n_cols = 3\n",
        "        n_rows = (len(sensores) + n_cols - 1) // n_cols\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 4 * n_rows))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        print(f\"Analisando {len(sensores)} sensores...\\n\")\n",
        "\n",
        "        i = -1  # Initialize i to handle empty sensores list\n",
        "        for i, sensor in enumerate(sensores):\n",
        "            # Dados das duas classes\n",
        "            data_norm = df_normal[sensor].dropna()\n",
        "            data_fail = df_falha[sensor].dropna()\n",
        "\n",
        "            # --- A. ESTATÍSTICA ---\n",
        "            # 1. Diferença de Médias\n",
        "            mean_diff = abs(data_fail.mean() - data_norm.mean())\n",
        "\n",
        "            # 2. Razão de Variância (Quantas vezes a vibração aumentou?)\n",
        "            # Adicionamos um epsilon pequeno para evitar divisão por zero\n",
        "            var_ratio = data_fail.var() / (data_norm.var() + 1e-9)\n",
        "\n",
        "            # 3. Teste Kolmogorov-Smirnov (Poder de Separação Geral)\n",
        "            # statistic: 0 a 1 (quanto maior, melhor separa as classes)\n",
        "            ks_stat, p_value = ks_2samp(data_norm, data_fail)\n",
        "\n",
        "            stats_list.append({\n",
        "                'Sensor': sensor,\n",
        "                'KS_Statistic': ks_stat, # Principal métrica de separação\n",
        "                'Variance_Ratio': var_ratio,\n",
        "                'Mean_Diff': mean_diff,\n",
        "                'P_Value': p_value\n",
        "            })\n",
        "\n",
        "            # --- B. VISUALIZAÇÃO (Violin Plot) ---\n",
        "            # Criamos um mini-df temporário para o seaborn\n",
        "            df_temp = pd.DataFrame({\n",
        "                'Valor': np.concatenate([data_norm, data_fail]),\n",
        "                'Estado': ['Normal'] * len(data_norm) + ['Falha'] * len(data_fail)\n",
        "            })\n",
        "\n",
        "            sns.violinplot(data=df_temp, x='Estado', y='Valor', ax=axes[i],\n",
        "                           palette={'Normal': '#1f77b4', 'Falha': '#d62728'}, split=False, hue='Estado')\n",
        "\n",
        "            axes[i].set_title(f'{sensor}\\nKS Stat: {ks_stat:.3f}', fontsize=10, fontweight='bold')\n",
        "            axes[i].set_xlabel('')\n",
        "            axes[i].set_ylabel('')\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "        # Remover axes vazios se houver\n",
        "        for j in range(i + 1, len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # --- C. TABELA DE RANKING ---\n",
        "        df_stats = pd.DataFrame(stats_list)\n",
        "        # Ordenar pelo KS Statistic (Melhor separador primeiro)\n",
        "        df_stats = df_stats.sort_values(by='KS_Statistic', ascending=False).reset_index(drop=True)\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(f\"{anomalia} - RANKING DE IMPORTÂNCIA DOS SENSORES (Baseado em KS-Test)\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"KS_Statistic: 1.0 = Separação Perfeita | 0.0 = Indistinguível\")\n",
        "        print(\"Variance_Ratio: > 1.0 = Falha aumentou a variabilidade\")\n",
        "        print(\"-\" * 60)\n",
        "        print(df_stats[['Sensor', 'KS_Statistic', 'Variance_Ratio', 'Mean_Diff']])\n",
        "\n",
        "        return df_stats\n",
        "\n",
        "    def plotSensors(self, dfPlot, step=1, suptitle=\"SENSORES AO LONGO DE 1 MINUTO\\n\",\n",
        "                    startTimeIdx=None, endTimeIdx=None):\n",
        "        \"\"\"\n",
        "        Plota os sensores (acelerômetro, giroscópio e magnetômetro) ao longo do tempo\n",
        "\n",
        "        Args:\n",
        "            dfPlot: DataFrame com os dados\n",
        "            step: Passo para amostragem dos dados\n",
        "            suptitle: Título do gráfico\n",
        "            startTimeIdx: Índice inicial (ou None para usar o primeiro)\n",
        "            endTimeIdx: Índice final (ou None para usar o último)\n",
        "        \"\"\"\n",
        "        df = dfPlot.iloc[::step]\n",
        "        fig = plt.figure(figsize=(25, 15))\n",
        "\n",
        "        if not any((startTimeIdx, endTimeIdx)):\n",
        "            startTimeIdx, endTimeIdx = df['time'].iloc[0], df['time'].iloc[-1]\n",
        "        else:\n",
        "            startTimeIdx, endTimeIdx = df['time'].iloc[startTimeIdx], df['time'].iloc[endTimeIdx]\n",
        "\n",
        "        # Helper to plot three axes in the same subplot\n",
        "        def plotSensorsSameGraph(ax, cols, title, x=\"time\"):\n",
        "            for col in cols:\n",
        "                mask = (df['time'] >= startTimeIdx) & (df['time'] < endTimeIdx)\n",
        "                ax.plot(df[mask][x], df[mask][col], label=col)\n",
        "\n",
        "            ax.set_title(title, fontsize=18)\n",
        "            ax.set_xlabel(x)\n",
        "            ax.set_ylabel(\"value\")\n",
        "            ax.legend(loc='lower left')\n",
        "\n",
        "        # === Subplots ===\n",
        "        ax1 = fig.add_subplot(3, 1, 1)\n",
        "        plotSensorsSameGraph(ax1,\n",
        "                             cols=[\"accX\", \"accY\", \"accZ\"],\n",
        "                             title=\"Accelerometer (X, Y, Z)\")\n",
        "\n",
        "        ax2 = fig.add_subplot(3, 1, 2)\n",
        "        plotSensorsSameGraph(ax2,\n",
        "                             cols=[\"gyroZ\", \"gyroX\", \"gyroY\"],\n",
        "                             title=\"Gyroscope (X, Y, Z)\")\n",
        "\n",
        "        ax3 = fig.add_subplot(3, 1, 3)\n",
        "        plotSensorsSameGraph(ax3,\n",
        "                             cols=[\"magZ\", \"magY\", \"magX\"],\n",
        "                             title=\"Magnetometer (X, Y, Z)\")\n",
        "        plt.suptitle(suptitle, fontsize='18')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def aplicar_filtro_savgol(self, df, cols=None, window_length=8, polyorder=2):\n",
        "        \"\"\"\n",
        "        Aplica o filtro Savitzky-Golay para suavizar os sinais dos sensores\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame com os dados\n",
        "            cols: Lista de colunas para aplicar o filtro (None = todas as colunas de sensores)\n",
        "            window_length: Tamanho da janela do filtro\n",
        "            polyorder: Ordem do polinômio\n",
        "\n",
        "        Returns:\n",
        "            DataFrame com as colunas suavizadas (_smooth)\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        if cols is None:\n",
        "            cols = [\"accX\", \"accY\", \"accZ\", \"gyroX\", \"gyroY\", \"gyroZ\", \"magX\", \"magY\", \"magZ\"]\n",
        "\n",
        "        for col in cols:\n",
        "            if col in df.columns:\n",
        "                df[col + \"_smooth\"] = savgol_filter(df[col], window_length=window_length, polyorder=polyorder)\n",
        "\n",
        "        print(f\"Filtro Savitzky-Golay aplicado em {len(cols)} colunas\")\n",
        "        return df\n",
        "\n",
        "    def plot_raw_vs_smooth(self, df, step=10):\n",
        "        \"\"\"\n",
        "        Plota comparação entre dados brutos e suavizados\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame com dados brutos e suavizados (_smooth)\n",
        "            step: Passo para amostragem dos dados\n",
        "        \"\"\"\n",
        "        sensor_groups = [\n",
        "            (\"Accelerometer\", [\"accX\", \"accY\", \"accZ\"]),\n",
        "            (\"Gyroscope\", [\"gyroX\", \"gyroY\", \"gyroZ\"]),\n",
        "            (\"Magnetometer\", [\"magX\", \"magY\", \"magZ\"])\n",
        "        ]\n",
        "\n",
        "        fig, axes = plt.subplots(3, 3, figsize=(32, 24))\n",
        "        df = df.iloc[::step]\n",
        "\n",
        "        for row, (title, cols) in enumerate(sensor_groups):\n",
        "            for col, axis in enumerate(cols):\n",
        "                ax = axes[row][col]\n",
        "\n",
        "                if axis not in df.columns:\n",
        "                    continue\n",
        "\n",
        "                raw = df[axis]\n",
        "                smooth_col = axis + \"_smooth\"\n",
        "\n",
        "                if smooth_col in df.columns:\n",
        "                    smooth = df[smooth_col]\n",
        "                    ax.plot(df[\"time\"], raw, label=\"raw\", alpha=0.35)\n",
        "                    ax.plot(df[\"time\"], smooth, label=\"smooth\", linewidth=2)\n",
        "                else:\n",
        "                    ax.plot(df[\"time\"], raw, label=\"raw\")\n",
        "\n",
        "                ax.set_title(f\"{title} — {axis}\", fontsize=16)\n",
        "                ax.set_xlabel(\"time\")\n",
        "                ax.set_ylabel(\"value\")\n",
        "                ax.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def executar_pipeline_completo(self, mostrar_diagnostico=False, mostrar_analise_univariada=True,\n",
        "                                   mostrar_sensores=False, mostrar_savgol=False, aplicar_filtro=False):\n",
        "        \"\"\"\n",
        "        Executa o pipeline completo de pré-processamento e análise\n",
        "\n",
        "        Args:\n",
        "            mostrar_diagnostico: Se True, mostra diagnóstico de pré-processamento\n",
        "            mostrar_analise_univariada: Se True, executa análise univariada\n",
        "            mostrar_sensores: Se True, plota visualização dos sensores\n",
        "            aplicar_filtro: Se True, aplica filtro Savitzky-Golay\n",
        "\n",
        "        Returns:\n",
        "            dict com os resultados do pipeline\n",
        "        \"\"\"\n",
        "        if self.df_normal is None or self.df_faulty is None:\n",
        "            raise ValueError(\"Carregue os dados primeiro usando carregar_e_preparar_dados()\")\n",
        "\n",
        "        resultados = {}\n",
        "\n",
        "        # 1. Aplicar resampling\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ETAPA 1: RESAMPLING\")\n",
        "        print(\"=\"*60)\n",
        "        self.aplicar_resampling()\n",
        "        resultados['df_normal_resampled'] = self.df_normal_resampled\n",
        "        resultados['df_faulty_resampled'] = self.df_faulty_resampled\n",
        "\n",
        "        # 2. Aplicar normalização\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ETAPA 2: NORMALIZAÇÃO\")\n",
        "        print(\"=\"*60)\n",
        "        scaled_data = self.aplicar_normalizacao()\n",
        "        resultados['scaled_data'] = {\n",
        "            'robust': (self.df_normal_robust_scaled, self.df_faulty_robust_scaled),\n",
        "            'standard': (self.df_normal_standard_scaled, self.df_faulty_standard_scaled)\n",
        "        }\n",
        "\n",
        "        # 3. Diagnóstico de pré-processamento\n",
        "        if mostrar_diagnostico:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"ETAPA 3: DIAGNÓSTICO DE PRÉ-PROCESSAMENTO\")\n",
        "            print(\"=\"*60)\n",
        "            print(\"\\nDiagnóstico com RobustScaler:\")\n",
        "            self.diagnostico_preprocessamento(\n",
        "                self.df_normal,\n",
        "                self.df_normal_resampled,\n",
        "                self.df_normal_robust_scaled,\n",
        "                col_sensor='accZ',\n",
        "                scaler_name='RobustScaler'\n",
        "            )\n",
        "            print(\"\\nDiagnóstico com StandardScaler:\")\n",
        "            self.diagnostico_preprocessamento(\n",
        "                self.df_normal,\n",
        "                self.df_normal_resampled,\n",
        "                self.df_normal_standard_scaled,\n",
        "                col_sensor='accZ',\n",
        "                scaler_name='StandardScaler'\n",
        "            )\n",
        "\n",
        "        # 4. Análise univariada\n",
        "        if mostrar_analise_univariada:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"ETAPA 4: ANÁLISE UNIVARIADA\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            for name, anomaly_resampled in self.faultydfs_resampled.items():\n",
        "                print(\"\\n\"+\"=\"*60)\n",
        "                print(f'ANOMALIA: {name}')\n",
        "                print(\"=\"*60)\n",
        "                df_stats = self.analise_univariada_alvo(self.df_normal_resampled, anomaly_resampled, name)\n",
        "\n",
        "        # 5. Visualização de sensores\n",
        "        if mostrar_sensores:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"ETAPA 5: VISUALIZAÇÃO DE SENSORES\")\n",
        "            print(\"=\"*60)\n",
        "            self.plotSensors(self.df_normal, startTimeIdx=20050, endTimeIdx=20050 + 60*10)\n",
        "\n",
        "        # 6. Aplicar filtro Savitzky-Golay\n",
        "        if aplicar_filtro:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"ETAPA 6: APLICAÇÃO DE FILTRO SAVITZKY-GOLAY\")\n",
        "            print(\"=\"*60)\n",
        "            df_filtered = self.aplicar_filtro_savgol(self.df_normal)\n",
        "            if mostrar_savgol: self.plot_raw_vs_smooth(df_filtered)\n",
        "            resultados['df_filtered'] = df_filtered\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"PIPELINE COMPLETO FINALIZADO!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "\n",
        "# Exemplo de uso\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Instancia a classe EDA\n",
        "#     eda = EDA()\n",
        "\n",
        "#     # Carrega e prepara os dados\n",
        "#     df_normal, df_faulty = eda.carregar_e_preparar_dados()\n",
        "\n",
        "    # Executa a análise exploratória completa\n",
        "    # eda.executar_analise_completa()\n",
        "\n",
        "    # Pipeline completo de pré-processamento (método integrado na classe)\n",
        "    # resultados = eda.executar_pipeline_completo(\n",
        "    #     mostrar_diagnostico=True,\n",
        "    #     mostrar_analise_univariada=True,\n",
        "    #     mostrar_sensores=True,\n",
        "    #     aplicar_filtro=True\n",
        "    # )\n",
        "\n",
        "    # print(resultados)\n",
        "\n",
        "    # OU executar etapas individuais:\n",
        "    # eda.aplicar_resampling()\n",
        "    # eda.aplicar_normalizacao()\n",
        "    # eda.diagnostico_preprocessamento(eda.df_normal, eda.df_normal_resampled,\n",
        "    #                                   eda.df_normal_robust_scaled, col_sensor='accZ')\n",
        "    # df_stats = eda.analise_univariada_alvo()\n",
        "    # eda.plotSensors(eda.df_normal, startTimeIdx=20050, endTimeIdx=20050 + 60*10)\n",
        "    # df_filtered = eda.aplicar_filtro_savgol(eda.df_normal)\n",
        "    # eda.plot_raw_vs_smooth(df_filtered)\n"
      ],
      "metadata": {
        "id": "UzIloSzdSQbx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pylab import normal\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.core import apply\n",
        "from scipy.stats import kurtosis, skew\n",
        "import sklearn\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import time\n",
        "df=None\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    eda= EDA()\n",
        "\n",
        "    def __init__(self,arquivo_normal= \"IMU_10Hz.csv\", arquivos_anomalos=[\"IMU_hitting_platform.csv\"], filtro_savgol=True):\n",
        "        \"\"\"Classe que carrega e prepara os dados. Ao ser instanciada, apenas fará\n",
        "        preparações que não serão customizadas (isto é, sem hiperparâmetros).\n",
        "\n",
        "        Methods:\n",
        "            preprocessar_df: Preprocessa completamnete um df, incluindo hiperparâmetros\n",
        "\n",
        "            preprocessar_todos: Preprocessa todos os datasets carregados por esta classe\"\"\"\n",
        "\n",
        "\n",
        "        self.normal: pd.DataFrame = None #type: ignore\n",
        "        self.anomalos: list[pd.DataFrame] = []\n",
        "        self.anomalo_nomes: list[str] = []\n",
        "\n",
        "        self.normal_splits: list[np.ndarray] = []\n",
        "        self.anomalo_splits: list[list[np.ndarray]] = []\n",
        "\n",
        "\n",
        "        eda= Preprocessing.eda\n",
        "        normal = pd.read_csv(eda.dataset.dataset_path + arquivo_normal)\n",
        "        anomalos = [pd.read_csv(eda.dataset.dataset_path + fname) for fname in arquivos_anomalos]\n",
        "\n",
        "        cols = list(set(normal.columns).difference(set([\"label\", 'name'])))\n",
        "        normal = normal[cols]\n",
        "        normal['time'] = normal['time'].map(lambda x: x/1e6) #type:ignore\n",
        "        if filtro_savgol:\n",
        "            normal = Preprocessing.eda.aplicar_filtro_savgol(normal)\n",
        "\n",
        "\n",
        "        # Corrige os anômalos e atualiza a lista\n",
        "        novos_anomalos = []\n",
        "        for anomalo in anomalos:\n",
        "            if filtro_savgol:\n",
        "                anomalo = Preprocessing.eda.aplicar_filtro_savgol(anomalo)\n",
        "            cols = list(set(anomalo.columns).difference(set([\"label\", 'name'])))\n",
        "            anomalo['time'] = anomalo['time'].map(lambda x: x/1e6)\n",
        "            anomalo = anomalo[cols]  # Garante que 'time' está presente\n",
        "            novos_anomalos.append(anomalo)\n",
        "        anomalos = novos_anomalos\n",
        "\n",
        "\n",
        "        def merge_smooth_columns(df):\n",
        "            # for each column, if it contains _smooth, replace the original column with it\n",
        "            for col in df.columns:\n",
        "                if col.endswith('_smooth'):\n",
        "                    original_col = col[:-7]\n",
        "                    df[original_col] = df[col]\n",
        "                    df.drop(columns=[col], inplace=True)\n",
        "            return df\n",
        "        normal = merge_smooth_columns(normal)\n",
        "        anomalos = [merge_smooth_columns(anomalo) for anomalo in anomalos]\n",
        "\n",
        "\n",
        "        normal = eda.resampling_and_interpolate(normal, arquivo_normal)\n",
        "        anomalos = [eda.resampling_and_interpolate(anomalo, nome) for anomalo,nome in zip(anomalos,arquivos_anomalos)]\n",
        "\n",
        "        normal_cols = list(normal.columns)\n",
        "        anomalos = [anomalo[normal_cols] for anomalo in anomalos] #evita um problema de ordem nas colunas\n",
        "\n",
        "        self.normal = normal\n",
        "        self.anomalos = anomalos #type:ignore\n",
        "        self.anomalo_nomes = arquivos_anomalos\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _getFixedWindows(df:pd.DataFrame|np.ndarray, length, overlap):\n",
        "        #drop incomplete implícito\n",
        "        arr: np.ndarray = df.values if isinstance(df, pd.DataFrame) else df\n",
        "        step = length - overlap\n",
        "        n = arr.shape[0]\n",
        "\n",
        "        starts = range(0, n - length + 1, step)\n",
        "        windows = np.stack([arr[s:s+length] for s in starts], axis=0) #semelhante a np.array\n",
        "\n",
        "        return windows\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def __preprocessar_DL__(df:pd.DataFrame, test_splits=[0.0,0.1,0.9], window_size=60, window_overlap=10, scaler=None, fit_scaler=False, stratifyCol=None) -> list[np.ndarray]:\n",
        "        \"\"\"Executa o preprocessamento completo de um df , sem exibir nada\n",
        "        Args:\n",
        "            window_size: tamanho da janela em samples continuas\n",
        "            window_overlap: interseção entre uma janela e a seguinte ou à antecessora\n",
        "\n",
        "        Returns:\n",
        "            df_train, df_val, df_test, df_val, df_test: Windows já achatadas,\n",
        "                com dados já normalizados, normalizados e limpos, com feature engineering pronto.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        df_train, df_val,df_test = Preprocessing._train_test_split(df, *test_splits,stratifyCol=stratifyCol)\n",
        "        if stratifyCol:\n",
        "            #assume that we want to remove thje label (causa bug?)\n",
        "            df_train = df_train.drop(columns=[stratifyCol]) if df_train is not None and len(df_train) else None\n",
        "            df_val = df_val.drop(columns=[stratifyCol])\n",
        "            df_test = df_test.drop(columns=[stratifyCol])\n",
        "        if scaler is None:\n",
        "            scaler = RobustScaler()\n",
        "            fit_scaler = True\n",
        "        dftrain_exists =  (df_train is not None and len(df_train))\n",
        "        if fit_scaler and dftrain_exists:\n",
        "            scaler.fit(df_train)\n",
        "\n",
        "\n",
        "        df_train =  pd.DataFrame(\n",
        "            scaler.transform(df_train),\n",
        "            columns=df_train.columns) if dftrain_exists > 0 else df_train\n",
        "        df_val = pd.DataFrame(scaler.transform(df_val), columns=df_val.columns)\n",
        "        df_test = pd.DataFrame(scaler.transform(df_test), columns=df_test.columns)\n",
        "\n",
        "        if dftrain_exists: df_train_w = Preprocessing._getFixedWindows(df_train, window_size, window_overlap)\n",
        "        df_val_w = Preprocessing._getFixedWindows(df_val,  window_size, window_overlap)\n",
        "        df_test_w  = Preprocessing._getFixedWindows(df_test,  window_size, window_overlap)\n",
        "\n",
        "\n",
        "        if dftrain_exists: df_train_flat = df_train_w.reshape(df_train_w.shape[0], -1) #type:ignore\n",
        "        df_val_flat = df_val_w.reshape(df_val_w.shape[0], -1)\n",
        "        df_test_flat  = df_test_w.reshape(df_test_w.shape[0], -1)\n",
        "\n",
        "        return [df_train_flat  if dftrain_exists else None, df_val_flat, df_test_flat] #type:ignore\n",
        "\n",
        "    @staticmethod\n",
        "    def _train_test_split(\n",
        "        df: pd.DataFrame,\n",
        "        trainPer=0.6,\n",
        "        valPer=0.05,\n",
        "        testPer=0.35,\n",
        "        stratifyCol=None\n",
        "    ) -> tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n",
        "        assert trainPer + valPer + testPer <= 1.0\n",
        "        n = len(df)\n",
        "\n",
        "        if stratifyCol is None:\n",
        "            train_end = int(n * trainPer)\n",
        "            val_end   = train_end + int(n * valPer)\n",
        "\n",
        "            traindf = df.iloc[:train_end].copy() if trainPer else None\n",
        "            valdf   = df.iloc[train_end:val_end].copy() if valPer else None\n",
        "            testdf  = df.iloc[val_end:val_end + int(n * testPer)].copy()\n",
        "\n",
        "            return (traindf, valdf, testdf) #type:ignore\n",
        "\n",
        "        train_idx = []\n",
        "        val_idx = []\n",
        "        test_idx = []\n",
        "\n",
        "        for label, group in df.groupby(stratifyCol, sort=False):\n",
        "            idx = group.index.to_numpy()\n",
        "            m = len(idx)\n",
        "\n",
        "            t_end = int(m * trainPer)\n",
        "            v_end = t_end + int(m * valPer)\n",
        "\n",
        "            train_idx.append(idx[:t_end])\n",
        "            val_idx.append(idx[t_end:v_end])\n",
        "            test_idx.append(idx[v_end:v_end + int(m * testPer)])\n",
        "\n",
        "        # Merge and restore temporal order\n",
        "        train_idx = np.sort(np.concatenate(train_idx)) if trainPer else None\n",
        "        val_idx   = np.sort(np.concatenate(val_idx)) if valPer else None\n",
        "        test_idx  = np.sort(np.concatenate(test_idx))\n",
        "\n",
        "        traindf = df.loc[train_idx].copy() if trainPer else None\n",
        "        valdf   = df.loc[val_idx].copy() if valPer else None\n",
        "        testdf  = df.loc[test_idx].copy()\n",
        "\n",
        "        return traindf, valdf, testdf #type:ignore\n",
        "\n",
        "    from scipy.stats import kurtosis, skew\n",
        "    @staticmethod\n",
        "    def __add_point_engineered_features__(df:pd.DataFrame) -> pd.DataFrame:\n",
        "        #add point only features\n",
        "        # sens_jerk e sens_norm\n",
        "        sensors = \"acc gyro mag\".split(' ')\n",
        "        df = df.copy()\n",
        "        # for sensAxis in base_features:\n",
        "        #     df[f\"{sensAxis}_jerk\"] = df[sensAxis].diff()\n",
        "        #jerk removido por variância irrisória\n",
        "        for sens in sensors:\n",
        "            df[f\"{sens}_norm\"] = np.sqrt(df[f\"{sens}X\"]**2 + df[f\"{sens}Y\"]**2 + df[f\"{sens}Z\"]**2)\n",
        "\n",
        "        return df\n",
        "    @staticmethod\n",
        "    def _make_windows(df, length, overlap, drop_incomplete=True):\n",
        "        arr = df.values  # (T, C)\n",
        "        step = length - overlap\n",
        "        n = arr.shape[0]\n",
        "\n",
        "        stops = n - length + 1 if drop_incomplete else n\n",
        "        starts = range(0, stops, step)\n",
        "\n",
        "        windows = []\n",
        "        for s in starts:\n",
        "            w = arr[s:s + length]\n",
        "            if w.shape[0] == length:\n",
        "                windows.append(w)\n",
        "\n",
        "        return np.stack(windows, axis=0)  # (N_windows, L, C)\n",
        "\n",
        "    @staticmethod\n",
        "    def __window_feature_engineering__(windows, feature_names, stats=(\"mean\", \"std\", \"ptp\", \"kurtosis\", \"crest\", \"dom_freq\")):\n",
        "        \"\"\"\n",
        "        Extracts statistical features from time-series windows for traditional ML models.\n",
        "\n",
        "        Args:\n",
        "            windows (np.array): Shape (N_samples, Window_Size, N_sensors)\n",
        "            feature_names (list): List of sensor names strings (e.g., ['accX', 'accY'...])\n",
        "            stats (tuple): List of stats to compute.\n",
        "\n",
        "        Returns:\n",
        "            X_feat (np.array): Shape (N_samples, N_features)\n",
        "            names (list): List of feature names\n",
        "        \"\"\"\n",
        "        feats = []\n",
        "        names = []\n",
        "\n",
        "        # 1. Mean (General Position/Bias)\n",
        "        if \"mean\" in stats:\n",
        "            feats.append(np.mean(windows, axis=1))\n",
        "            names += [f\"mean_{c}\" for c in feature_names]\n",
        "\n",
        "        # 2. Standard Deviation (Vibration Energy)\n",
        "        if \"std\" in stats:\n",
        "            feats.append(np.std(windows, axis=1))\n",
        "            names += [f\"std_{c}\" for c in feature_names]\n",
        "\n",
        "        # 3. RMS (Total Energy - redundancy with Mean/Std, but useful for physics)\n",
        "        if \"rms\" in stats:\n",
        "            rms = np.sqrt(np.mean(windows**2, axis=1))\n",
        "            feats.append(rms)\n",
        "            names += [f\"rms_{c}\" for c in feature_names]\n",
        "\n",
        "        # 4. Peak-to-Peak (Amplitude of Shocks - critical for Bumps)\n",
        "        if \"ptp\" in stats:\n",
        "            feats.append(np.ptp(windows, axis=1))\n",
        "            names += [f\"ptp_{c}\" for c in feature_names]\n",
        "\n",
        "        # 5. Kurtosis (Impulsiveness - critical for Hits/Earthquakes)\n",
        "        if \"kurtosis\" in stats:\n",
        "            # Fisher=False makes normal distribution = 3.0.\n",
        "            # Often easier to use Fisher=True (normal = 0.0) for ML centering.\n",
        "            k = kurtosis(windows, axis=1, fisher=True)\n",
        "            feats.append(k)\n",
        "            names += [f\"kurt_{c}\" for c in feature_names]\n",
        "\n",
        "        # 6. Skewness (Asymmetry - useful for directional crashes)\n",
        "        if \"skew\" in stats:\n",
        "            s = skew(windows, axis=1)\n",
        "            feats.append(s)\n",
        "            names += [f\"skew_{c}\" for c in feature_names]\n",
        "\n",
        "        # 7. Crest Factor (Impact Indicator - Peak / RMS)\n",
        "        if \"crest\" in stats:\n",
        "            peak = np.max(np.abs(windows), axis=1)\n",
        "            rms = np.sqrt(np.mean(windows**2, axis=1))\n",
        "            # Add small epsilon to avoid division by zero\n",
        "            crest = peak / (rms + 1e-9)\n",
        "            feats.append(crest)\n",
        "            names += [f\"crest_{c}\" for c in feature_names]\n",
        "\n",
        "        # 8. Dominant Frequency (Resonance - useful for Heavy Weight detection)\n",
        "        if \"dom_freq\" in stats:\n",
        "            # Perform FFT along the time axis (axis 1)\n",
        "            fft_vals = np.fft.rfft(windows, axis=1)\n",
        "            fft_freq = np.fft.rfftfreq(windows.shape[1])\n",
        "\n",
        "            # Find index of max magnitude (ignoring DC component at index 0)\n",
        "            magnitudes = np.abs(fft_vals)\n",
        "            magnitudes[:, 0, :] = 0  # Zero out DC component\n",
        "\n",
        "            dom_indices = np.argmax(magnitudes, axis=1) # Shape (N, C)\n",
        "\n",
        "            # Map indices to actual frequencies\n",
        "            dom_freqs = fft_freq[dom_indices]\n",
        "\n",
        "            feats.append(dom_freqs)\n",
        "            names += [f\"domfreq_{c}\" for c in feature_names]\n",
        "\n",
        "        X_feat = np.concatenate(feats, axis=1)  # (N, Total_Features)\n",
        "        return X_feat, names\n",
        "\n",
        "    @staticmethod\n",
        "    def __non_DL_feature_engineering_pipeline__(ds, WINDOW_SIZE=40, WINDOW_OVERLAP=10):\n",
        "        #somente do ponto\n",
        "        ds_pe = Preprocessing.__add_point_engineered_features__(ds)\n",
        "\n",
        "        ds_w = Preprocessing._make_windows(ds_pe, WINDOW_SIZE, WINDOW_OVERLAP)\n",
        "\n",
        "        #somente da janela\n",
        "        ds_feat, feat_names = Preprocessing.__window_feature_engineering__(\n",
        "            ds_w,\n",
        "            feature_names=ds_pe.columns,\n",
        "            )\n",
        "        return ds_feat, feat_names\n",
        "\n",
        "    @staticmethod\n",
        "    def __PCA_normalize__(X_train_masked, X_val_masked=None, X_test_masked=None, variance_threshold=0.95):\n",
        "        \"\"\"\n",
        "        Faz scaling e PCA nos splits. recebe os datasets ja somente com as features usadas\n",
        "        \"\"\"\n",
        "        from sklearn.decomposition import PCA\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "        scaler = RobustScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_masked)\n",
        "\n",
        "        # 2. Fit PCA\n",
        "        pca = PCA(n_components=variance_threshold)\n",
        "        X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "        print(f\"PCA:\")\n",
        "        print(f\"  - Features antes : {X_train_masked.shape[1]}\")\n",
        "        print(f\"  - Features após: {X_train_pca.shape[1]}\")\n",
        "        print(f\"  - Variância explicada: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "        X_val_pca = None\n",
        "        X_test_pca = None\n",
        "\n",
        "        if X_val_masked is not None:\n",
        "            X_val_scaled = scaler.transform(X_val_masked)\n",
        "            X_val_pca = pca.transform(X_val_scaled)\n",
        "\n",
        "        if X_test_masked is not None:\n",
        "            X_test_scaled = scaler.transform(X_test_masked)\n",
        "            X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "        return pca, scaler, X_train_pca, X_val_pca, X_test_pca\n",
        "\n",
        "    @staticmethod\n",
        "    def __preprocessar_non_DL__(train:pd.DataFrame|np.ndarray, val, test, pca=None, scaler=None,mask=None,WINDOW_SIZE=60,WINDOW_OVERLAP=10):\n",
        "\n",
        "        train,names = Preprocessing.__non_DL_feature_engineering_pipeline__(train, WINDOW_SIZE,WINDOW_OVERLAP)\n",
        "\n",
        "        val,names = Preprocessing.__non_DL_feature_engineering_pipeline__(val, WINDOW_SIZE,WINDOW_OVERLAP)\n",
        "        test,names = Preprocessing.__non_DL_feature_engineering_pipeline__(test, WINDOW_SIZE,WINDOW_OVERLAP)\n",
        "        assert len(train.shape) == 2 and  len(val.shape) == 2 and len(test.shape) == 2\n",
        "\n",
        "        if mask is None:\n",
        "            var = train.var(axis=0)\n",
        "            VAR_THRESHOLD = 1e-5\n",
        "            keep_var = var > VAR_THRESHOLD\n",
        "\n",
        "            # Apply variance mask first\n",
        "            train_var = train[:, keep_var]\n",
        "\n",
        "            corr = np.corrcoef(train_var, rowvar=False)\n",
        "            CORR_THRESHOLD = 0.95\n",
        "            to_drop = set()\n",
        "            for i in range(corr.shape[0]):\n",
        "                for j in range(i+1, corr.shape[0]):\n",
        "                    if abs(corr[i,j]) > CORR_THRESHOLD:\n",
        "                        to_drop.add(j)\n",
        "\n",
        "            keep_corr = [i not in to_drop for i in range(corr.shape[0])]\n",
        "\n",
        "            # Final mask: first variance, then correlation\n",
        "            mask = np.zeros(train.shape[1], dtype=bool)\n",
        "            mask[np.where(keep_var)[0][keep_corr]] = True\n",
        "        def apply_mask(arr):\n",
        "            return arr[:, mask]\n",
        "\n",
        "        if not pca:\n",
        "            assert not scaler\n",
        "            a,b,c = apply_mask(train), apply_mask(val),apply_mask(test)\n",
        "            pca, scaler, train_pca,val_pca,test_pca = Preprocessing.__PCA_normalize__(\n",
        "                a,b,c,variance_threshold=0.95\n",
        "            )\n",
        "        else:\n",
        "            assert pca and scaler\n",
        "            a,b,c = apply_mask(train), apply_mask(val),apply_mask(test)\n",
        "            train_pca,val_pca,test_pca = (pca.transform(scaler.transform(a)),\n",
        "                pca.transform(scaler.transform(b)),\n",
        "                pca.transform(scaler.transform(c)))\n",
        "\n",
        "        return pca,scaler,mask,train_pca,val_pca,test_pca\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def preprocessar_todos_non_deepLearning(self, aplicar_savgol=True, train_splits = [0.6,0.2,0.2], test_splits=[0.0,0.5,0.5], window_size=60, window_overlap=10):\n",
        "        assert aplicar_savgol #motivos de compatibilidade de api\n",
        "        anomalos = [ano.copy() for ano in self.anomalos]\n",
        "        for i,ano in enumerate(anomalos):\n",
        "            ano['label'] = i\n",
        "        anomalo_total = pd.concat(anomalos)\n",
        "\n",
        "        xtrain,xval,xtest = Preprocessing._train_test_split(self.normal,*train_splits)\n",
        "\n",
        "        _,anom_val, anom_test = Preprocessing._train_test_split(anomalo_total,trainPer=0,valPer=0.5,testPer=0.5)#type:ignore\n",
        "        anom_val = anom_val.drop(columns=['label'])\n",
        "        anom_test = anom_test.drop(columns=['label'])\n",
        "\n",
        "        #feature enginering e windowing\n",
        "\n",
        "        pca,scaler,mask,xtrain,xval,xtest = Preprocessing.__preprocessar_non_DL__(xtrain,xval,\n",
        "            xtest,pca=None,scaler=None,mask=None,\n",
        "            WINDOW_SIZE=window_size,WINDOW_OVERLAP=window_overlap)\n",
        "        pca,scaler,mask,_,anom_val,anom_test = Preprocessing.__preprocessar_non_DL__(anom_val,anom_val,\n",
        "            anom_test,pca=pca,scaler=scaler,mask=mask,\n",
        "            WINDOW_SIZE=window_size,WINDOW_OVERLAP=window_overlap)\n",
        "\n",
        "        self.normal_splits = [xtrain,xval,xtest] #type:ignore\n",
        "        self.anomalo_splits=[anom_val,anom_test]#type:ignore\n",
        "\n",
        "\n",
        "    def preprocessar_todos_deepLearning(self, aplicar_savgol=True, train_splits = [0.6,0.2,0.2], test_splits=[0.0,0.5,0.5], window_size=60, window_overlap=10):\n",
        "        \"\"\"Preprocessa todos os datasets carregados por esta classe e os coloca em\n",
        "            self.normal_splits e self.anomalo_splits\n",
        "\n",
        "        Args:\n",
        "            aplicar_savgol: Se deve aplicar o filtro de Savgolay\n",
        "            window_size: tamanho da janela em samples continuas\n",
        "            window_overlap: interseção entre uma janela e a seguinte ou à antecessora\n",
        "\n",
        "        Returns:\n",
        "            dict: dicionário com chaves 'normal' e nomes dos datasets anômalos,\n",
        "                  cada um contendo uma lista [df_train, df_val, df_test]\n",
        "        \"\"\"\n",
        "        assert aplicar_savgol #motivos de compatibilidade de api\n",
        "        anomalos = [ano.copy() for ano in self.anomalos]\n",
        "        for i,ano in enumerate(anomalos):\n",
        "            ano['label'] = i\n",
        "        anomalo_total = pd.concat(anomalos)\n",
        "\n",
        "\n",
        "\n",
        "        scaler = RobustScaler()\n",
        "        self.normal_splits = Preprocessing.__preprocessar_DL__(self.normal, train_splits,\n",
        "                                                         window_size,\n",
        "                                                         window_overlap,\n",
        "                                                         scaler=scaler,\n",
        "                                                         fit_scaler=True)\n",
        "\n",
        "        self.anomalo_splits = Preprocessing.__preprocessar_DL__(anomalo_total,  test_splits, window_size,\n",
        "                                             window_overlap,scaler=scaler,\n",
        "                                             fit_scaler=False, stratifyCol='label')[1:]\n",
        "\n",
        "        return None\n",
        "    @staticmethod\n",
        "    def resizeFlattenedWindow( flattened_windows:np.ndarray, new_window_size:int, window_overlap:int, dimensionsPerSample=9) -> np.ndarray:\n",
        "        \"\"\"Redimensiona  as janelas (já) achatadas para um novo tamanho\"\"\"\n",
        "        assert (len(flattened_windows.shape) == 2)\n",
        "        if window_overlap >= new_window_size:\n",
        "            window_overlap = new_window_size//2\n",
        "\n",
        "        samplesPerWindow = flattened_windows.shape[1]//dimensionsPerSample\n",
        "        samples = flattened_windows.reshape(-1, dimensionsPerSample)\n",
        "\n",
        "        newWindows = Preprocessing._getFixedWindows(samples, new_window_size, window_overlap)\n",
        "        flattenedNewWindows = newWindows.reshape(newWindows.shape[0], -1)\n",
        "        return flattenedNewWindows\n",
        "\n",
        "\n",
        "\n",
        "# pp = Preprocessing()\n",
        "# pp.preprocessar_todos_non_deepLearning()\n",
        "# print(pp.anomalo_splits[1].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy-HK67-Saco",
        "outputId": "bcb31609-3aec-4261-8202-69ee9f9a8e6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'industrial-robotic-arm-imu-data-casper-1-and-2' dataset.\n",
            "--- CARREGAMENTO MANUAL DE CENÁRIOS ---\n",
            "Lendo Base Normal...\n",
            "Lendo Base de Falhas...\n",
            "-> Adicionado: IMU_hitting_platform.csv (14967 linhas)\n",
            "-> Adicionado: IMU_hitting_arm.csv (11924 linhas)\n",
            "-> Adicionado: IMU_extra_weigth.csv (10885 linhas)\n",
            "-> Adicionado: IMU_earthquake.csv (11409 linhas)\n",
            "1. Normal processado: 874937 linhas divididas.\n",
            "Falha IMU_hitting_platform.csv processado\n",
            "Falha IMU_hitting_arm.csv processado\n",
            "Falha IMU_extra_weigth.csv processado\n",
            "Falha IMU_earthquake.csv processado\n",
            "============================================================\n",
            "DATASET PRONTO:\n",
            "-> Dados Normais: 874937 linhas\n",
            "-> Dados de Falha:  24591 linhas (Total de 4 tipos de defeito)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WIms-NQ1cFq-"
      },
      "outputs": [],
      "source": [
        "from abc import ABCMeta, abstractmethod\n",
        "from typing import Callable, Optional\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "class Trainer:\n",
        "    @abstractmethod\n",
        "    def __init__(self, **kwargs) -> None:\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def TrainerFit(self, X_train:np.ndarray, Y_train:Optional[np.ndarray]) -> None:\n",
        "        \"\"\"Treina\n",
        "        Args:\n",
        "            Y_train: provavelmente você nao deve usar esse parametro\"\"\"\n",
        "        ...\n",
        "\n",
        "    @abstractmethod\n",
        "    def TrainerPred(self, X:np.ndarray) -> np.ndarray:\n",
        "        ...\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, **kwargs) -> None:\n",
        "        ...\n",
        "\n",
        "    def evaluate(self, Y: np.ndarray, Y_pred: np.ndarray, threshold=None, *args, **kwargs) -> tuple[float, dict]:\n",
        "        assert threshold is None\n",
        "        # Threshold not implemented yet\n",
        "        \"\"\"Evaluates the model's predictions, returning a main metric and other auxiliary metrics\"\"\"\n",
        "\n",
        "        accuracy = accuracy_score(Y, Y_pred)\n",
        "        precision = precision_score(Y, Y_pred, zero_division=0)\n",
        "        recall = recall_score(Y, Y_pred, zero_division=0)\n",
        "        f1 = f1_score(Y, Y_pred, zero_division=0)\n",
        "        auc = roc_auc_score(Y, Y_pred)\n",
        "\n",
        "        return auc, {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"auc\": auc\n",
        "        }\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "class BaseHyperParamTuner(ABC):\n",
        "    def __init__(self,\n",
        "                 modelTrainerParams: dict[str, list],\n",
        "                 X_train: np.ndarray,\n",
        "                 X_val: np.ndarray,\n",
        "                 X_test: np.ndarray,\n",
        "                 anom_val: np.ndarray,\n",
        "                 anom_test: np.ndarray):\n",
        "\n",
        "        self.param_grid = modelTrainerParams\n",
        "\n",
        "        # NOTE: For StaticFeaturesTuner, these should be RAW data (or point-features).\n",
        "        # For DeepLearningTuner, these are likely already flattened windows.\n",
        "        self.X_train = X_train\n",
        "        self.X_val = X_val\n",
        "        self.X_test = X_test\n",
        "        self.anom_val = anom_val\n",
        "        self.anom_test = anom_test\n",
        "\n",
        "        self.best_params: dict | None = None\n",
        "        self.best_score: float = -np.inf\n",
        "        self.best_model: Trainer | None = None\n",
        "        self.results: list[dict] = []\n",
        "        self._bestResult = {\"EMPTY\":0}\n",
        "\n",
        "        self._expandedModelGrid = self._expand_grid(self.param_grid)\n",
        "\n",
        "    @staticmethod\n",
        "    def _expand_grid(grid: dict[str, list]) -> list[dict]:\n",
        "        if not grid:\n",
        "            return [{}]\n",
        "        keys = list(grid.keys())\n",
        "        values = [v if isinstance(v, (list, tuple, np.ndarray)) else [v] for v in grid.values()]\n",
        "        return [dict(zip(keys, combo)) for combo in itertools.product(*values)]\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_window_grid(self) -> list[dict]:\n",
        "        \"\"\"Define iteration over window parameters.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def _prepare_data(self, window_params: dict) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Transform data based on window params.\n",
        "        This is called inside the OUTER loop, ensuring expensive operations run only once per config.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def tune(self, Trainer_factory: Callable[..., Trainer], evaluator: Evaluator) -> None:\n",
        "        idx = 0\n",
        "\n",
        "        window_grid = self._get_window_grid()\n",
        "\n",
        "        #caro pra carmaba\n",
        "        for window_params in window_grid:\n",
        "            print(f\"Processing Window Config: {window_params}\")\n",
        "\n",
        "\n",
        "            Xtr, validationX, validationLabels = self._prepare_data(window_params)\n",
        "\n",
        "\n",
        "            for modelParams in self._expandedModelGrid:\n",
        "                idx+=1\n",
        "                trainer = Trainer_factory(**modelParams)\n",
        "\n",
        "                # Fit\n",
        "                trainer.TrainerFit(Xtr, None)\n",
        "\n",
        "                # Predict\n",
        "                anom_val_pred = trainer.TrainerPred(validationX)\n",
        "\n",
        "                # Evaluate\n",
        "                score, metrics = evaluator.evaluate(validationLabels, anom_val_pred)\n",
        "\n",
        "                record = {\n",
        "                    \"score\": score,\n",
        "                    \"metrics\": metrics,\n",
        "                    \"window_params\": window_params,\n",
        "                    \"model_params\": modelParams,\n",
        "                    \"tuner_type\": self.__class__.__name__\n",
        "                }\n",
        "                self.results.append(record)\n",
        "\n",
        "                if score > self.best_score:\n",
        "                    print(record[\"model_params\"])\n",
        "                    self.best_score = score\n",
        "                    self.best_params = record\n",
        "                    self.best_model = trainer\n",
        "                    self._bestResult = record\n",
        "\n",
        "\n",
        "class DeepLearningTuner(BaseHyperParamTuner):\n",
        "    def __init__(self, window_params: dict[str, list], **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.window_params = window_params\n",
        "        self._expandedWindowGrid = self._expand_grid(self.window_params)\n",
        "\n",
        "    def _get_window_grid(self) -> list[dict]:\n",
        "        return self._expandedWindowGrid\n",
        "\n",
        "    def _prepare_data(self, window_params: dict):\n",
        "        #APENAS REDIMENSIONA SEM LIDAR COM FEATURES\n",
        "        Xtr = Preprocessing.resizeFlattenedWindow(self.X_train, **window_params)\n",
        "        Xval = Preprocessing.resizeFlattenedWindow(self.X_val, **window_params)\n",
        "        anomVal = Preprocessing.resizeFlattenedWindow(self.anom_val, **window_params)\n",
        "\n",
        "        validationX = np.concatenate((Xval, anomVal), axis=0)\n",
        "        validationLabels = np.concatenate((np.ones(Xval.shape[0]), np.zeros(anomVal.shape[0])))\n",
        "\n",
        "        return Xtr, validationX, validationLabels\n",
        "\n",
        "def recompute_preprocessing(pp):\n",
        "    # Adicione *args para capturar (e ignorar) X_train, X_val, anom_val\n",
        "    # Ou defina explicitamente: def a(X_tr, X_val, anom, new_window_size, ...)\n",
        "    def a(*args, new_window_size, window_overlap, dimensionsPerSample=None):\n",
        "        pp.preprocessar_todos_deepLearning(window_size=new_window_size,window_overlap=window_overlap)\n",
        "        return *(pp.normal_splits)[:2],pp.anomalo_splits[0]\n",
        "    return a\n",
        "\n",
        "\n",
        "\n",
        "class StaticFeaturesTuner(BaseHyperParamTuner):\n",
        "    \"\"\"\n",
        "    Tuner for Traditional ML models where windowing involves expensive Feature Engineering.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                window_params: dict[str, list],\n",
        "                 feature_engineering_fn: Callable[..., tuple[np.ndarray, np.ndarray, np.ndarray]]=recompute_preprocessing,  #type:ignore\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            feature_engineering_fn: Function that accepts (X_train, X_val, anom_val, **window_params)\n",
        "                                    and returns (X_train_feats, X_val_feats, anom_val_feats).\n",
        "                                    The inputs X_train etc. will be the RAW data stored in this class.\n",
        "            window_params: Grid of parameters to pass to the function (e.g. {'window_size': [10, 20]}).\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.feature_engineering_fn = feature_engineering_fn\n",
        "        self.window_params = window_params\n",
        "        self._expandedWindowGrid = self._expand_grid(self.window_params)\n",
        "\n",
        "    def _get_window_grid(self) -> list[dict]:\n",
        "        return self._expandedWindowGrid\n",
        "\n",
        "    def _prepare_data(self, window_params: dict):\n",
        "\n",
        "        # 1. Call the user-provided function to generate features from RAW data\n",
        "        # This is the \"expensive\" step, executed only once per window config via the Base class loop.\n",
        "        X_train_feats, X_val_feats, anom_val_feats = self.feature_engineering_fn(\n",
        "            self.X_train,\n",
        "            self.X_val,\n",
        "            self.anom_val,\n",
        "            **window_params\n",
        "        )\n",
        "\n",
        "        # 2. Prepare validation concatenation (Standard logic)\n",
        "        validationX = np.concatenate((X_val_feats, anom_val_feats), axis=0)\n",
        "        validationLabels = np.concatenate((\n",
        "            np.ones(X_val_feats.shape[0]),\n",
        "            np.zeros(anom_val_feats.shape[0])\n",
        "        ))\n",
        "\n",
        "        return X_train_feats, validationX, validationLabels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dd98d6c",
        "outputId": "3a8651c1-99c8-4c2f-b212-30b16dd19ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GMMTrainer class defined successfully.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "\n",
        "class GMMTrainer(Trainer):\n",
        "    \"\"\"GMMTrainer for anomaly detection using Gaussian Mixture Models.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_components: int = 2,\n",
        "        covariance_type: str = \"full\",\n",
        "        reg_covar: float = 1e-06,\n",
        "        threshold_percentile: float = 5,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the GMMTrainer with GMM parameters and an anomaly threshold percentile.\n",
        "\n",
        "        Args:\n",
        "            n_components: The number of mixture components.\n",
        "            covariance_type: String describing the type of covariance parameters to use.\n",
        "            reg_covar: Non-negative regularization added to the diagonal of covariance.\n",
        "            threshold_percentile: Percentile of log-likelihoods to set as the anomaly threshold.\n",
        "        \"\"\"\n",
        "        self.gmm = GaussianMixture(\n",
        "            n_components=n_components,\n",
        "            covariance_type=covariance_type,\n",
        "            reg_covar=reg_covar,\n",
        "            **kwargs\n",
        "        )\n",
        "        self.threshold: float | None = None\n",
        "        self.threshold_percentile = threshold_percentile\n",
        "\n",
        "    def TrainerFit(self, X_train: np.ndarray, Y_train: np.ndarray | None = None) -> None:\n",
        "        \"\"\"        Trains the GMM model and determines the anomaly threshold.\n",
        "\n",
        "        Args:\n",
        "            X_train: Normal training data.\n",
        "            Y_train: Not used in this unsupervised anomaly detection method.\n",
        "        \"\"\"\n",
        "        self.gmm.fit(X_train)\n",
        "        log_likelihoods = self.gmm.score_samples(X_train)\n",
        "        self.threshold = np.percentile(log_likelihoods, self.threshold_percentile)\n",
        "        print(f\"Anomaly threshold set at {self.threshold:.4f} (based on {self.threshold_percentile}th percentile).\")\n",
        "\n",
        "    def TrainerPred(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predicts anomalies (0) or normal (1) for new data based on the trained GMM and threshold.\n",
        "\n",
        "        Args:\n",
        "            X: Data to classify.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: An array of classifications (1 for normal, 0 for anomalous).\n",
        "        \"\"\"\n",
        "        if self.threshold is None:\n",
        "            raise ValueError(\"Model not fitted. Call TrainerFit first.\")\n",
        "\n",
        "        log_likelihoods = self.gmm.score_samples(X)\n",
        "        # Classify as normal (1) if log-likelihood >= threshold, else anomalous (0)\n",
        "        predictions = (log_likelihoods >= self.threshold).astype(int)\n",
        "        return predictions\n",
        "\n",
        "print(\"GMMTrainer class defined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32891371",
        "outputId": "dc17944a-9272-4dac-8d0d-43b7c498fdae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtro Savitzky-Golay aplicado em 9 colunas\n",
            "Filtro Savitzky-Golay aplicado em 9 colunas\n",
            "\n",
            "Fazendo Resampling e Interpolate de IMU_10Hz.csv\n",
            "Buracos gerados pelo alinhamento: 44766\n",
            "Resampling concluído. Novo shape: (876347, 10)\n",
            "\n",
            "Fazendo Resampling e Interpolate de IMU_hitting_platform.csv\n",
            "Buracos gerados pelo alinhamento: 900\n",
            "Resampling concluído. Novo shape: (14992, 10)\n",
            "PCA:\n",
            "  - Features antes : 65\n",
            "  - Features após: 32\n",
            "  - Variância explicada: 0.9543\n"
          ]
        }
      ],
      "source": [
        "pp = Preprocessing()\n",
        "pp.preprocessar_todos_non_deepLearning(aplicar_savgol=True, train_splits=[0.7, 0.1, 0.2], test_splits=[0.0, 0.5, 0.5])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0Ec1RBPoSHqm"
      },
      "outputs": [],
      "source": [
        "X_train = pp.normal_splits[0]\n",
        "X_val = pp.normal_splits[1]\n",
        "X_test = pp.normal_splits[2]\n",
        "\n",
        "anom_val = pp.anomalo_splits[0]\n",
        "anom_test = pp.anomalo_splits[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b47c4a8b",
        "outputId": "d22aac03-faf2-4492-ad77-5fa99f78894a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Window Config: {'new_window_size': 30, 'window_overlap': 0, 'dimensionsPerSample': 60}\n",
            "Anomaly threshold set at -985.9116 (based on 1th percentile).\n",
            "{'n_components': 1, 'covariance_type': 'diag', 'reg_covar': 1e-05, 'threshold_percentile': 1}\n",
            "Anomaly threshold set at -970.3180 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -949.9687 (based on 5th percentile).\n",
            "Anomaly threshold set at -813.8804 (based on 1th percentile).\n",
            "Anomaly threshold set at -800.9970 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -787.5919 (based on 5th percentile).\n",
            "Anomaly threshold set at -803.1392 (based on 1th percentile).\n",
            "{'n_components': 3, 'covariance_type': 'diag', 'reg_covar': 1e-05, 'threshold_percentile': 1}\n",
            "Anomaly threshold set at -797.4322 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -783.9876 (based on 5th percentile).\n",
            "Anomaly threshold set at -802.7577 (based on 1th percentile).\n",
            "{'n_components': 4, 'covariance_type': 'diag', 'reg_covar': 1e-05, 'threshold_percentile': 1}\n",
            "Anomaly threshold set at -790.3662 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -775.9296 (based on 5th percentile).\n",
            "Processing Window Config: {'new_window_size': 30, 'window_overlap': 15, 'dimensionsPerSample': 60}\n",
            "Anomaly threshold set at -985.5591 (based on 1th percentile).\n",
            "Anomaly threshold set at -970.0236 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -950.0018 (based on 5th percentile).\n",
            "Anomaly threshold set at -814.0985 (based on 1th percentile).\n",
            "Anomaly threshold set at -801.2326 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -787.6912 (based on 5th percentile).\n",
            "Anomaly threshold set at -810.5148 (based on 1th percentile).\n",
            "Anomaly threshold set at -797.7610 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -784.1914 (based on 5th percentile).\n",
            "Anomaly threshold set at -805.5689 (based on 1th percentile).\n",
            "Anomaly threshold set at -752.7920 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -731.1104 (based on 5th percentile).\n",
            "Processing Window Config: {'new_window_size': 60, 'window_overlap': 0, 'dimensionsPerSample': 60}\n",
            "Anomaly threshold set at -1913.7397 (based on 1th percentile).\n",
            "{'n_components': 1, 'covariance_type': 'diag', 'reg_covar': 1e-05, 'threshold_percentile': 1}\n",
            "Anomaly threshold set at -1871.0222 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -1771.1049 (based on 5th percentile).\n",
            "Anomaly threshold set at -1620.0229 (based on 1th percentile).\n",
            "Anomaly threshold set at -1587.6368 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -1546.2263 (based on 5th percentile).\n",
            "Anomaly threshold set at -1593.9746 (based on 1th percentile).\n",
            "Anomaly threshold set at -1561.6990 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -1525.0094 (based on 5th percentile).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomaly threshold set at -1574.9371 (based on 1th percentile).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomaly threshold set at -1555.0939 (based on 2.5th percentile).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomaly threshold set at -1517.0496 (based on 5th percentile).\n",
            "Processing Window Config: {'new_window_size': 60, 'window_overlap': 15, 'dimensionsPerSample': 60}\n",
            "Anomaly threshold set at -1913.4202 (based on 1th percentile).\n",
            "Anomaly threshold set at -1871.2401 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -1768.8959 (based on 5th percentile).\n",
            "Anomaly threshold set at -1622.1022 (based on 1th percentile).\n",
            "Anomaly threshold set at -1587.6730 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -1547.4193 (based on 5th percentile).\n",
            "Anomaly threshold set at -1593.5859 (based on 1th percentile).\n",
            "Anomaly threshold set at -1562.4958 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -1524.6926 (based on 5th percentile).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomaly threshold set at -1588.7050 (based on 1th percentile).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anomaly threshold set at -1555.4326 (based on 2.5th percentile).\n",
            "Anomaly threshold set at -1516.5860 (based on 5th percentile).\n",
            "fim\n",
            "\n",
            "Best Hyperparameters Found:\n",
            "  Window Params: {'new_window_size': 60, 'window_overlap': 0, 'dimensionsPerSample': 60}\n",
            "  Model Params: {'n_components': 1, 'covariance_type': 'diag', 'reg_covar': 1e-05, 'threshold_percentile': 1}\n",
            "Best Validation Score: 0.9765\n",
            "Best scores: {'score': np.float64(0.9765491270112975), 'metrics': {'accuracy': 0.955008210180624, 'precision': 1.0, 'recall': 0.953098254022595, 'f1': 0.9759859772129711, 'auc': np.float64(0.9765491270112975)}, 'window_params': {'new_window_size': 60, 'window_overlap': 0, 'dimensionsPerSample': 60}, 'model_params': {'n_components': 1, 'covariance_type': 'diag', 'reg_covar': 1e-05, 'threshold_percentile': 1}, 'tuner_type': 'StaticFeaturesTuner'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/mixture/_base.py:269: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "window_hyperparameters = {\n",
        "    \"new_window_size\": [30, 60],\n",
        "    \"window_overlap\": [0, 15],\n",
        "    \"dimensionsPerSample\": [X_train.shape[1] // pp.normal.shape[1]]\n",
        "}\n",
        "\n",
        "model_hyperparameters = {\n",
        "    \"n_components\": [1, 2, 3, 4],\n",
        "    \"covariance_type\": [\"diag\", \"full\"],\n",
        "    \"reg_covar\": [1e-05],\n",
        "    \"threshold_percentile\": [1, 2.5, 5]\n",
        "}\n",
        "\n",
        "evaluator = Evaluator()\n",
        "\n",
        "tuner = StaticFeaturesTuner(\n",
        "    window_params=window_hyperparameters,\n",
        "    modelTrainerParams=model_hyperparameters,\n",
        "    feature_engineering_fn=recompute_preprocessing(pp),\n",
        "    X_train=X_train,\n",
        "    X_val=X_val,\n",
        "    X_test=X_test,\n",
        "    anom_val=anom_val,\n",
        "    anom_test=anom_test\n",
        ")\n",
        "\n",
        "tuner.tune(GMMTrainer, evaluator)\n",
        "\n",
        "print(\"fim\")\n",
        "\n",
        "if tuner.best_params:\n",
        "    print(\"\\nBest Hyperparameters Found:\")\n",
        "    print(f\"  Window Params: {tuner.best_params['window_params']}\")\n",
        "    print(f\"  Model Params: {tuner.best_params['model_params']}\")\n",
        "    print(f\"Best Validation Score: {tuner.best_score:.4f}\")\n",
        "    print(f\"Best scores: {tuner._bestResult}\")\n",
        "else:\n",
        "    print(\"No best parameters found.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "best_gmm = tuner.best_model\n",
        "\n",
        "y_pred_test = best_gmm.TrainerPred(X_test)\n",
        "y_pred_anom  = best_gmm.TrainerPred(anom_test)\n",
        "\n",
        "y_pred = np.concatenate((y_pred_test, y_pred_anom))\n",
        "y_true = np.concatenate((np.ones_like(y_pred_test, dtype=int), np.zeros_like(y_pred_anom, dtype=int)))\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec_anom = precision_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "rec_anom = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "f1_anom = f1_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision (anomaly as +): {prec_anom:.4f}\")\n",
        "print(f\"Recall    (anomaly as +): {rec_anom:.4f}\")\n",
        "print(f\"F1-score  (anomaly as +): {f1_anom:.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, target_names=['anomaly','normal'], zero_division=0))\n",
        "\n",
        "try:\n",
        "    ll_test = best_gmm.gmm.score_samples(X_test)\n",
        "    ll_anom = best_gmm.gmm.score_samples(anom_test)\n",
        "    print(f\"\\nMean log-likelihood (normal test): {np.mean(ll_test):.4f}\")\n",
        "    print(f\"Mean log-likelihood (anomalous test): {np.mean(ll_anom):.4f}\")\n",
        "\n",
        "    scores_test = -ll_test\n",
        "    scores_anom = -ll_anom\n",
        "    scores = np.concatenate((scores_test, scores_anom))\n",
        "    y_true_anom = (y_true == 0).astype(int)  # 1 = anomaly for roc/ap\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_true_anom, scores)\n",
        "    except ValueError:\n",
        "        roc_auc = float(\"nan\")\n",
        "    try:\n",
        "        ap = average_precision_score(y_true_anom, scores)\n",
        "    except ValueError:\n",
        "        ap = float(\"nan\")\n",
        "\n",
        "    print(f\"ROC AUC (anomaly positive): {roc_auc:.4f}\")\n",
        "    print(f\"Average Precision (PR AUC): {ap:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Não foi possível obter log-likelihoods do GMM para calcular scores contínuos:\", e)\n",
        "    print(\"Verifique se o best_gmm tem o atributo .gmm (uma GaussianMixture treinada).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMcydMKl1o3Q",
        "outputId": "8d4b1695-210b-49b5-c99f-8a6546b03526"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix (rows=true, cols=pred):\n",
            " [[ 166    0]\n",
            " [ 708 3186]]\n",
            "Accuracy: 0.8256\n",
            "Precision (anomaly as +): 0.1899\n",
            "Recall    (anomaly as +): 1.0000\n",
            "F1-score  (anomaly as +): 0.3192\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anomaly       0.19      1.00      0.32       166\n",
            "      normal       1.00      0.82      0.90      3894\n",
            "\n",
            "    accuracy                           0.83      4060\n",
            "   macro avg       0.59      0.91      0.61      4060\n",
            "weighted avg       0.97      0.83      0.88      4060\n",
            "\n",
            "\n",
            "Mean log-likelihood (normal test): -1672.9433\n",
            "Mean log-likelihood (anomalous test): -12554031.9700\n",
            "ROC AUC (anomaly positive): 1.0000\n",
            "Average Precision (PR AUC): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "632e1f5f",
        "outputId": "71e23382-70f1-44f1-c5c8-61d1b182dd8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IsolationForestTrainer class defined successfully.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "\n",
        "class IsolationForestTrainer(Trainer):\n",
        "    \"\"\"IsolationForestTrainer for anomaly detection using Isolation Forest.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators: int = 100,\n",
        "        max_samples: str | float = \"auto\",\n",
        "        contamination: float | str = \"auto\",\n",
        "        random_state: int | None = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the IsolationForestTrainer with Isolation Forest parameters.\n",
        "\n",
        "        Args:\n",
        "            n_estimators: The number of base estimators in the ensemble.\n",
        "            max_samples: The number of samples to draw from X to train each base estimator.\n",
        "            contamination: The amount of contamination of the data set, i.e. the proportion of outliers in the data set.\n",
        "            random_state: Controls the pseudo-randomness of the estimator.\n",
        "            **kwargs: Additional parameters for sklearn.ensemble.IsolationForest.\n",
        "        \"\"\"\n",
        "        self.model = IsolationForest(\n",
        "            n_estimators=n_estimators,\n",
        "            max_samples=max_samples,\n",
        "            contamination=contamination,\n",
        "            random_state=random_state,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def TrainerFit(self, X_train: np.ndarray, Y_train: np.ndarray | None = None) -> None:\n",
        "        \"\"\"\n",
        "        Trains the Isolation Forest model.\n",
        "\n",
        "        Args:\n",
        "            X_train: Normal training data.\n",
        "            Y_train: Not used in this unsupervised anomaly detection method.\n",
        "        \"\"\"\n",
        "        print(\"Training IsolationForest model...\")\n",
        "        self.model.fit(X_train)\n",
        "        print(\"IsolationForest model trained successfully.\")\n",
        "\n",
        "    def TrainerPred(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predicts anomalies (0) or normal (1) for new data.\n",
        "\n",
        "        Args:\n",
        "            X: Data to classify.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: An array of classifications (1 for normal, 0 for anomalous).\n",
        "        \"\"\"\n",
        "        raw_predictions = self.model.predict(X)\n",
        "        # Convert Isolation Forest output (-1 for anomaly, 1 for normal) to Evaluator's format (0 for anomaly, 1 for normal)\n",
        "        predictions = np.where(raw_predictions == 1, 1, 0)\n",
        "        return predictions\n",
        "\n",
        "print(\"IsolationForestTrainer class defined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c00477c7",
        "outputId": "4298b9be-a4d0-4ec8-a050-1cc4be7e5d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Window Config: {'new_window_size': 30, 'window_overlap': 0, 'dimensionsPerSample': 60}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "{'n_estimators': 50, 'max_samples': 'auto', 'contamination': 'auto', 'random_state': 42}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "{'n_estimators': 50, 'max_samples': 0.25, 'contamination': 'auto', 'random_state': 42}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "{'n_estimators': 50, 'max_samples': 0.5, 'contamination': 'auto', 'random_state': 42}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "{'n_estimators': 100, 'max_samples': 0.5, 'contamination': 'auto', 'random_state': 42}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "{'n_estimators': 100, 'max_samples': 0.5, 'contamination': 0.1, 'random_state': 42}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Processing Window Config: {'new_window_size': 30, 'window_overlap': 15, 'dimensionsPerSample': 60}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "{'n_estimators': 50, 'max_samples': 0.5, 'contamination': 'auto', 'random_state': 42}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Processing Window Config: {'new_window_size': 60, 'window_overlap': 0, 'dimensionsPerSample': 60}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Processing Window Config: {'new_window_size': 60, 'window_overlap': 15, 'dimensionsPerSample': 60}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "{'n_estimators': 50, 'max_samples': 0.75, 'contamination': 'auto', 'random_state': 42}\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "Training IsolationForest model...\n",
            "IsolationForest model trained successfully.\n",
            "  Window Params: {'new_window_size': 60, 'window_overlap': 15, 'dimensionsPerSample': 60}\n",
            "  Model Params: {'n_estimators': 50, 'max_samples': 0.75, 'contamination': 'auto', 'random_state': 42}\n",
            "Best Validation Score: 0.7015\n"
          ]
        }
      ],
      "source": [
        "window_hyperparameters = {\n",
        "    \"new_window_size\": [30, 60],\n",
        "    \"window_overlap\": [0, 15],\n",
        "    \"dimensionsPerSample\": [X_train.shape[1] // pp.normal.shape[1]]\n",
        "}\n",
        "\n",
        "model_hyperparameters = {\n",
        "    \"n_estimators\": [50, 100, 150, 200, 250, 300],\n",
        "    \"max_samples\": [\"auto\", 0.25, 0.5, 0.75],\n",
        "    \"contamination\": [\"auto\", 0.01, 0.05, 0.1],\n",
        "    \"random_state\": [42]\n",
        "}\n",
        "\n",
        "evaluator = Evaluator()\n",
        "\n",
        "tuner = StaticFeaturesTuner(\n",
        "    window_params=window_hyperparameters,\n",
        "    modelTrainerParams=model_hyperparameters,\n",
        "    feature_engineering_fn=recompute_preprocessing(pp),\n",
        "    X_train=X_train,\n",
        "    X_val=X_val,\n",
        "    X_test=X_test,\n",
        "    anom_val=anom_val,\n",
        "    anom_test=anom_test\n",
        ")\n",
        "\n",
        "tuner.tune(IsolationForestTrainer, evaluator)\n",
        "\n",
        "print(f\"  Window Params: {tuner.best_params['window_params']}\")\n",
        "print(f\"  Model Params: {tuner.best_params['model_params']}\")\n",
        "print(f\"Best Validation Score: {tuner.best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tuner.best_model)\n",
        "tuner.best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbUT5JyBZvjB",
        "outputId": "93288943-6974-4908-954a-76158327d4c7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.IsolationForestTrainer object at 0x7fb59136f200>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': np.float64(0.701500918929957),\n",
              " 'metrics': {'accuracy': 0.8588669950738916,\n",
              "  'precision': 0.9775668679896462,\n",
              "  'recall': 0.8728813559322034,\n",
              "  'f1': 0.9222629222629223,\n",
              "  'auc': np.float64(0.701500918929957)},\n",
              " 'window_params': {'new_window_size': 60,\n",
              "  'window_overlap': 15,\n",
              "  'dimensionsPerSample': 60},\n",
              " 'model_params': {'n_estimators': 50,\n",
              "  'max_samples': 0.75,\n",
              "  'contamination': 'auto',\n",
              "  'random_state': 42},\n",
              " 'tuner_type': 'StaticFeaturesTuner'}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "best_iso = tuner.best_model\n",
        "\n",
        "y_pred_test = best_iso.TrainerPred(X_test)\n",
        "y_pred_anom = best_iso.TrainerPred(anom_test)\n",
        "\n",
        "y_pred = np.concatenate((y_pred_test, y_pred_anom))\n",
        "y_true = np.concatenate((np.ones_like(y_pred_test, dtype=int), np.zeros_like(y_pred_anom, dtype=int)))\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec_anom = precision_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "rec_anom = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "f1_anom = f1_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision (anomaly as +): {prec_anom:.4f}\")\n",
        "print(f\"Recall    (anomaly as +): {rec_anom:.4f}\")\n",
        "print(f\"F1-score  (anomaly as +): {f1_anom:.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, target_names=['anomaly','normal'], zero_division=0))\n",
        "\n",
        "\n",
        "if hasattr(best_iso, \"model\") and hasattr(best_iso.model, \"decision_function\"):\n",
        "    scores_test = -best_iso.model.decision_function(X_test)\n",
        "    scores_anom = -best_iso.model.decision_function(anom_test)\n",
        "    scores = np.concatenate((scores_test, scores_anom))\n",
        "    y_true_anom = (y_true == 0).astype(int)  # 1 = anomaly for sklearn metrics\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_true_anom, scores)\n",
        "    except ValueError:\n",
        "        roc_auc = float(\"nan\")\n",
        "    try:\n",
        "        ap = average_precision_score(y_true_anom, scores)\n",
        "    except ValueError:\n",
        "        ap = float(\"nan\")\n",
        "\n",
        "    print(f\"ROC AUC (anomaly positive): {roc_auc:.4f}\")\n",
        "    print(f\"Average Precision (PR AUC): {ap:.4f}\")\n",
        "else:\n",
        "    print(\"Modelo não expõe decision_function; não foi possível calcular ROC/PR AUC.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjBwVDq-mLu2",
        "outputId": "aa23839a-8dfa-47af-a603-0ed7d6c9e4dc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix (rows=true, cols=pred):\n",
            " [[  87   79]\n",
            " [ 498 3396]]\n",
            "Accuracy: 0.8579\n",
            "Precision (anomaly as +): 0.1487\n",
            "Recall    (anomaly as +): 0.5241\n",
            "F1-score  (anomaly as +): 0.2317\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anomaly       0.15      0.52      0.23       166\n",
            "      normal       0.98      0.87      0.92      3894\n",
            "\n",
            "    accuracy                           0.86      4060\n",
            "   macro avg       0.56      0.70      0.58      4060\n",
            "weighted avg       0.94      0.86      0.89      4060\n",
            "\n",
            "ROC AUC (anomaly positive): 0.7745\n",
            "Average Precision (PR AUC): 0.3212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7TOtNmfqLQ9u"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_cnn_autoencoder(\n",
        "    window_size: int,\n",
        "    n_sensors: int,\n",
        "    filters: list = (32, 16),\n",
        "    kernel_sizes: list = (3, 3),\n",
        "    pool_size: int = 2,\n",
        "    latent_channels: int = 8,\n",
        "    use_batchnorm: bool = True,\n",
        "):\n",
        "    assert len(filters) == len(kernel_sizes)\n",
        "    inp = layers.Input(shape=(window_size, n_sensors))\n",
        "\n",
        "    # Encoder\n",
        "    x = inp\n",
        "    for f, k in zip(filters, kernel_sizes):\n",
        "        x = layers.Conv1D(f, kernel_size=k, padding='same', activation='relu')(x)\n",
        "        if use_batchnorm:\n",
        "            x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(pool_size=pool_size, padding='same')(x)\n",
        "\n",
        "    # Latent\n",
        "    x = layers.Conv1D(latent_channels, kernel_size=1, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = layers.UpSampling1D(size=pool_size)(x)\n",
        "    for f, k in zip(filters[::-1], kernel_sizes[::-1]):\n",
        "        x = layers.Conv1D(f, kernel_size=k, padding='same', activation='relu')(x)\n",
        "        if use_batchnorm:\n",
        "            x = layers.BatchNormalization()(x)\n",
        "\n",
        "    decoded = layers.Conv1D(n_sensors, kernel_size=1, padding='same', activation=None)(x)\n",
        "\n",
        "    model = models.Model(inp, decoded)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70967fb3",
        "outputId": "fd11b54c-fd5c-4fec-8ef6-1f5b9d5ab2c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features per timestep (n_sensors): 10\n",
            "Window length: 60\n",
            "Shape of X_train_reshaped: (11684, 60, 10)\n",
            "Shape of X_val_reshaped: (3894, 60, 10)\n",
            "Shape of anom_val_reshaped: (166, 60, 10)\n"
          ]
        }
      ],
      "source": [
        "WINDOW_SIZE_DL = 60\n",
        "WINDOW_OVERLAP_DL = 15\n",
        "\n",
        "pp.preprocessar_todos_deepLearning(window_size=WINDOW_SIZE_DL, window_overlap=WINDOW_OVERLAP_DL)\n",
        "\n",
        "X_train = pp.normal_splits[0]\n",
        "X_val = pp.normal_splits[1]\n",
        "anom_val = pp.anomalo_splits[0]\n",
        "\n",
        "n_features_per_timestep = pp.normal.shape[1]\n",
        "window_length = WINDOW_SIZE_DL\n",
        "\n",
        "# Reshape the flattened windows back into 3D format (num_samples, window_length, n_features_per_timestep)\n",
        "X_train_reshaped = X_train.reshape(-1, window_length, n_features_per_timestep)\n",
        "X_val_reshaped = X_val.reshape(-1, window_length, n_features_per_timestep)\n",
        "anom_val_reshaped = anom_val.reshape(-1, window_length, n_features_per_timestep)\n",
        "\n",
        "print(f\"Number of features per timestep (n_sensors): {n_features_per_timestep}\")\n",
        "print(f\"Window length: {window_length}\")\n",
        "print(f\"Shape of X_train_reshaped: {X_train_reshaped.shape}\")\n",
        "print(f\"Shape of X_val_reshaped: {X_val_reshaped.shape}\")\n",
        "print(f\"Shape of anom_val_reshaped: {anom_val_reshaped.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74543db1",
        "outputId": "e1e5a88f-5f07-4534-cc13-41325ea0a84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - loss: 19.5279 - val_loss: 10.9729\n",
            "Epoch 2/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 7.9083 - val_loss: 4.7716\n",
            "Epoch 3/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 4.0962 - val_loss: 2.7320\n",
            "Epoch 4/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 2.9565 - val_loss: 2.2188\n",
            "Epoch 5/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 2.5028 - val_loss: 1.9380\n",
            "Epoch 6/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 2.2636 - val_loss: 1.7598\n",
            "Epoch 7/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 2.1119 - val_loss: 1.6416\n",
            "Epoch 8/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 1.9847 - val_loss: 1.5399\n",
            "Epoch 9/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.8960 - val_loss: 1.4687\n",
            "Epoch 10/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.8237 - val_loss: 1.4308\n",
            "Epoch 11/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.7685 - val_loss: 1.4050\n",
            "Epoch 12/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.7204 - val_loss: 1.3778\n",
            "Epoch 13/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.6794 - val_loss: 1.3511\n",
            "Epoch 14/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.6437 - val_loss: 1.3222\n",
            "Epoch 15/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 1.6120 - val_loss: 1.2937\n",
            "Epoch 16/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.5823 - val_loss: 1.2768\n",
            "Epoch 17/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 1.5531 - val_loss: 1.2503\n",
            "Epoch 18/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 1.5214 - val_loss: 1.2158\n",
            "Epoch 19/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.4922 - val_loss: 1.1869\n",
            "Epoch 20/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.4669 - val_loss: 1.1609\n",
            "Epoch 21/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 1.4420 - val_loss: 1.1327\n",
            "Epoch 22/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.4200 - val_loss: 1.1168\n",
            "Epoch 23/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 1.4012 - val_loss: 1.1043\n",
            "Epoch 24/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 1.3845 - val_loss: 1.0941\n",
            "Epoch 25/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.3682 - val_loss: 1.0810\n",
            "Epoch 26/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 1.3543 - val_loss: 1.0707\n",
            "Epoch 27/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.3396 - val_loss: 1.0641\n",
            "Epoch 28/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.3255 - val_loss: 1.0514\n",
            "Epoch 29/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.3127 - val_loss: 1.0401\n",
            "Epoch 30/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 1.3007 - val_loss: 1.0335\n",
            "Epoch 31/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.2901 - val_loss: 1.0244\n",
            "Epoch 32/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 1.2799 - val_loss: 1.0199\n",
            "Epoch 33/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 1.2702 - val_loss: 1.0098\n",
            "Epoch 34/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 1.2614 - val_loss: 1.0084\n",
            "Epoch 35/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - loss: 1.2535 - val_loss: 1.0029\n",
            "Epoch 36/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 1.2453 - val_loss: 0.9973\n",
            "Epoch 37/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.2378 - val_loss: 0.9906\n",
            "Epoch 38/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 1.2300 - val_loss: 0.9889\n",
            "Epoch 39/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.2216 - val_loss: 0.9848\n",
            "Epoch 40/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.2135 - val_loss: 0.9827\n",
            "Epoch 41/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.2053 - val_loss: 0.9811\n",
            "Epoch 42/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1964 - val_loss: 0.9675\n",
            "Epoch 43/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 1.1873 - val_loss: 0.9597\n",
            "Epoch 44/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.1789 - val_loss: 0.9563\n",
            "Epoch 45/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 1.1711 - val_loss: 0.9464\n",
            "Epoch 46/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1639 - val_loss: 0.9433\n",
            "Epoch 47/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.1574 - val_loss: 0.9434\n",
            "Epoch 48/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 1.1511 - val_loss: 0.9382\n",
            "Epoch 49/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1447 - val_loss: 0.9378\n",
            "Epoch 50/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 1.1384 - val_loss: 0.9360\n",
            "Epoch 51/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 1.1320 - val_loss: 0.9325\n",
            "Epoch 52/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.1252 - val_loss: 0.9219\n",
            "Epoch 53/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 1.1180 - val_loss: 0.9180\n",
            "Epoch 54/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.1112 - val_loss: 0.9187\n",
            "Epoch 55/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 1.1023 - val_loss: 0.8984\n",
            "Epoch 56/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 1.0904 - val_loss: 0.8777\n",
            "Epoch 57/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 1.0802 - val_loss: 0.8735\n",
            "Epoch 58/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 1.0716 - val_loss: 0.8685\n",
            "Epoch 59/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 1.0636 - val_loss: 0.8691\n",
            "Epoch 60/60\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 1.0553 - val_loss: 0.8725\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fb56ffce690>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "cnn_autoencoder_model = build_cnn_autoencoder(\n",
        "    window_size=window_length,\n",
        "    n_sensors=n_features_per_timestep,\n",
        "    filters=[8, 6],\n",
        "    kernel_sizes=[5, 3],\n",
        "    pool_size=2,\n",
        "    latent_channels=4,\n",
        "    use_batchnorm=True\n",
        ")\n",
        "\n",
        "cnn_autoencoder_model.fit(\n",
        "    X_train_reshaped,\n",
        "    X_train_reshaped,\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val_reshaped, X_val_reshaped),\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66febb60",
        "outputId": "37f60cc3-9140-47d3-f197-8e1bb3102d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Autoencoder Reconstruction Error...\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Mean Reconstruction Error (MSE) for normal validation data: 0.8685\n",
            "Mean Reconstruction Error (MSE) for anomalous validation data: 14014.8165\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nEvaluating Autoencoder Reconstruction Error...\")\n",
        "X_val_pred = cnn_autoencoder_model.predict(X_val_reshaped)\n",
        "mse_normal = np.mean(np.square(X_val_reshaped - X_val_pred), axis=(1, 2))\n",
        "\n",
        "anom_val_pred = cnn_autoencoder_model.predict(anom_val_reshaped)\n",
        "mse_anomalous = np.mean(np.square(anom_val_reshaped - anom_val_pred), axis=(1, 2))\n",
        "\n",
        "print(f\"Mean Reconstruction Error (MSE) for normal validation data: {np.mean(mse_normal):.4f}\")\n",
        "print(f\"Mean Reconstruction Error (MSE) for anomalous validation data: {np.mean(mse_anomalous):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "anomaly_threshold = np.percentile(mse_normal, 99)  # NÃO 100\n",
        "\n",
        "y_pred_normal = (mse_normal <= anomaly_threshold).astype(int)   # normal se <= limiar\n",
        "y_pred_anomalous = (mse_anomalous <= anomaly_threshold).astype(int)\n",
        "\n",
        "y_pred = np.concatenate((y_pred_normal, y_pred_anomalous))\n",
        "y_true = np.concatenate((\n",
        "    np.ones_like(mse_normal, dtype=int),  # 1 = normal\n",
        "    np.zeros_like(mse_anomalous, dtype=int)  # 0 = anomaly\n",
        "))\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "prec_anom = precision_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "rec_anom = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "f1_anom  = f1_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision (anomaly as +): {prec_anom:.4f}\")\n",
        "print(f\"Recall    (anomaly as +): {rec_anom:.4f}\")\n",
        "print(f\"F1-score  (anomaly as +): {f1_anom:.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, target_names=['anomaly','normal'], zero_division=0))\n",
        "\n",
        "scores = np.concatenate((mse_normal, mse_anomalous))  # maior => mais anômalo\n",
        "y_true_anom = (y_true == 0).astype(int)               # 1 = anomaly\n",
        "\n",
        "roc_auc = roc_auc_score(y_true_anom, scores)\n",
        "ap = average_precision_score(y_true_anom, scores)    # PR AUC\n",
        "\n",
        "print(f\"ROC AUC (anomaly positive): {roc_auc:.4f}\")\n",
        "print(f\"Average Precision (PR AUC): {ap:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CXGQ6Z4b26H",
        "outputId": "afe817b1-7ac7-4bd5-d08e-0c9a6150ee61"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix:\n",
            " [[ 166    0]\n",
            " [  39 3855]]\n",
            "Accuracy: 0.9904\n",
            "Precision (anomaly as +): 0.8098\n",
            "Recall    (anomaly as +): 1.0000\n",
            "F1-score  (anomaly as +): 0.8949\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anomaly       0.81      1.00      0.89       166\n",
            "      normal       1.00      0.99      0.99      3894\n",
            "\n",
            "    accuracy                           0.99      4060\n",
            "   macro avg       0.90      0.99      0.94      4060\n",
            "weighted avg       0.99      0.99      0.99      4060\n",
            "\n",
            "ROC AUC (anomaly positive): 1.0000\n",
            "Average Precision (PR AUC): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pp.normal_splits[2]\n",
        "anom_test = pp.anomalo_splits[1]\n",
        "\n",
        "X_test_reshaped = X_test.reshape(-1, window_length, n_features_per_timestep)\n",
        "anom_test_reshaped = anom_test.reshape(-1, window_length, n_features_per_timestep)\n",
        "\n",
        "print(f\"Shape of X_test_reshaped: {X_test_reshaped.shape}\")\n",
        "print(f\"Shape of anom_test_reshaped: {anom_test_reshaped.shape}\")\n",
        "\n",
        "print(\"\\nEvaluating Autoencoder Reconstruction Error on Test Data...\")\n",
        "X_test_pred = cnn_autoencoder_model.predict(X_test_reshaped)\n",
        "mse_test_normal = np.mean(np.square(X_test_reshaped - X_test_pred), axis=(1, 2))\n",
        "\n",
        "anom_test_pred = cnn_autoencoder_model.predict(anom_test_reshaped)\n",
        "mse_test_anomalous = np.mean(np.square(anom_test_reshaped - anom_test_pred), axis=(1, 2))\n",
        "\n",
        "print(f\"Mean Reconstruction Error (MSE) for normal test data: {np.mean(mse_test_normal):.4f}\")\n",
        "print(f\"Mean Reconstruction Error (MSE) for anomalous test data: {np.mean(mse_test_anomalous):.4f}\")\n",
        "\n",
        "y_pred_test_normal = (mse_test_normal <= anomaly_threshold).astype(int)\n",
        "y_pred_test_anomalous = (mse_test_anomalous <= anomaly_threshold).astype(int)\n",
        "\n",
        "y_pred_test = np.concatenate((y_pred_test_normal, y_pred_test_anomalous))\n",
        "y_true_test = np.concatenate((\n",
        "    np.ones_like(mse_test_normal, dtype=int),  # 1 = normal\n",
        "    np.zeros_like(mse_test_anomalous, dtype=int)  # 0 = anomaly\n",
        "))\n",
        "\n",
        "acc_test = accuracy_score(y_true_test, y_pred_test)\n",
        "prec_anom_test = precision_score(y_true_test, y_pred_test, pos_label=0, zero_division=0)\n",
        "rec_anom_test = recall_score(y_true_test, y_pred_test, pos_label=0, zero_division=0)\n",
        "f1_anom_test  = f1_score(y_true_test, y_pred_test, pos_label=0, zero_division=0)\n",
        "cm_test = confusion_matrix(y_true_test, y_pred_test)\n",
        "\n",
        "print(\"\\n--- Test Set Performance ---\")\n",
        "print(\"Confusion matrix:\\n\", cm_test)\n",
        "print(f\"Accuracy: {acc_test:.4f}\")\n",
        "print(f\"Precision (anomaly as +): {prec_anom_test:.4f}\")\n",
        "print(f\"Recall    (anomaly as +): {rec_anom_test:.4f}\")\n",
        "print(f\"F1-score  (anomaly as +): {f1_anom_test:.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_true_test, y_pred_test, target_names=['anomaly','normal'], zero_division=0))\n",
        "\n",
        "scores_test = np.concatenate((mse_test_normal, mse_test_anomalous))  # maior => mais anômalo\n",
        "y_true_anom_test = (y_true_test == 0).astype(int)               # 1 = anomaly\n",
        "\n",
        "roc_auc_test = roc_auc_score(y_true_anom_test, scores_test)\n",
        "ap_test = average_precision_score(y_true_anom_test, scores_test)    # PR AUC\n",
        "\n",
        "print(f\"ROC AUC (anomaly positive): {roc_auc_test:.4f}\")\n",
        "print(f\"Average Precision (PR AUC): {ap_test:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzPMEXmHc_Qo",
        "outputId": "0328be25-395c-4100-aaa2-39d72de86a01"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_test_reshaped: (3894, 60, 10)\n",
            "Shape of anom_test_reshaped: (166, 60, 10)\n",
            "\n",
            "Evaluating Autoencoder Reconstruction Error on Test Data...\n",
            "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Mean Reconstruction Error (MSE) for normal test data: 1.0947\n",
            "Mean Reconstruction Error (MSE) for anomalous test data: 14016.3169\n",
            "\n",
            "--- Test Set Performance ---\n",
            "Confusion matrix:\n",
            " [[ 166    0]\n",
            " [ 176 3718]]\n",
            "Accuracy: 0.9567\n",
            "Precision (anomaly as +): 0.4854\n",
            "Recall    (anomaly as +): 1.0000\n",
            "F1-score  (anomaly as +): 0.6535\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     anomaly       0.49      1.00      0.65       166\n",
            "      normal       1.00      0.95      0.98      3894\n",
            "\n",
            "    accuracy                           0.96      4060\n",
            "   macro avg       0.74      0.98      0.82      4060\n",
            "weighted avg       0.98      0.96      0.96      4060\n",
            "\n",
            "ROC AUC (anomaly positive): 1.0000\n",
            "Average Precision (PR AUC): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "class AETrainer(Trainer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        # arquitetura\n",
        "        filters=(32,16),\n",
        "        kernel_sizes=(3,3),\n",
        "        pool_size=2,\n",
        "        latent_channels=8,\n",
        "        use_batchnorm=True,\n",
        "        # treino\n",
        "        batch_size=64,\n",
        "        epochs=50,\n",
        "        lr=1e-3,\n",
        "        patience=5,\n",
        "        threshold_percentile=95.0,\n",
        "        verbose=0,\n",
        "        window_length: int | None = None,\n",
        "        n_sensors: int | None = None,\n",
        "        random_seed: int | None = 42,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # arquitetura\n",
        "        self.filters = list(filters)\n",
        "        self.kernel_sizes = list(kernel_sizes)\n",
        "        self.pool_size = pool_size\n",
        "        self.latent_channels = latent_channels\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "\n",
        "        # treino\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.epochs = int(epochs)\n",
        "        self.lr = float(lr)\n",
        "        self.patience = int(patience)\n",
        "        self.threshold_percentile = float(threshold_percentile)\n",
        "        self.verbose = int(verbose)\n",
        "\n",
        "        self.window_length = window_length\n",
        "        self.n_sensors = n_sensors\n",
        "\n",
        "        self.model: tf.keras.Model | None = None\n",
        "        self.threshold: float | None = None\n",
        "        self.train_recon_err_ = None\n",
        "\n",
        "        if random_seed is not None:\n",
        "            np.random.seed(random_seed)\n",
        "            tf.random.set_seed(random_seed)\n",
        "\n",
        "    def _build_model(self, window_size, n_sensors):\n",
        "        inp = layers.Input(shape=(window_size, n_sensors))\n",
        "        x = inp\n",
        "        for f, k in zip(self.filters, self.kernel_sizes):\n",
        "            x = layers.Conv1D(f, kernel_size=k, padding='same', activation='relu')(x)\n",
        "            if self.use_batchnorm:\n",
        "                x = layers.BatchNormalization()(x)\n",
        "        x = layers.MaxPooling1D(pool_size=self.pool_size, padding='same')(x)\n",
        "        x = layers.Conv1D(self.latent_channels, kernel_size=1, padding='same', activation='relu')(x)\n",
        "        x = layers.UpSampling1D(size=self.pool_size)(x)\n",
        "        for f, k in zip(self.filters[::-1], self.kernel_sizes[::-1]):\n",
        "            x = layers.Conv1D(f, kernel_size=k, padding='same', activation='relu')(x)\n",
        "            if self.use_batchnorm:\n",
        "                x = layers.BatchNormalization()(x)\n",
        "        decoded = layers.Conv1D(n_sensors, kernel_size=1, padding='same', activation=None)(x)\n",
        "        model = models.Model(inp, decoded)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def TrainerFit(self, X_train_flat: np.ndarray, Y_train: np.ndarray | None = None) -> None:\n",
        "        \"\"\"\n",
        "        X_train_flat: (n_windows, window_length * n_sensors)  <- that's what Preprocessing.resizeFlattenedWindow returns\n",
        "        \"\"\"\n",
        "        print(\"Training Autoencoder\")\n",
        "        if X_train_flat.ndim != 2:\n",
        "            raise ValueError(\"X_train must be 2D flattened windows (n_windows, window_length * n_sensors)\")\n",
        "\n",
        "        if (self.window_length is None) or (self.n_sensors is None):\n",
        "\n",
        "            total_feats = X_train_flat.shape[1]\n",
        "            if total_feats % self.n_sensors == 0 and self.n_sensors is not None:\n",
        "                self.window_length = total_feats // self.n_sensors\n",
        "            else:\n",
        "                raise ValueError(\"window_length and n_sensors must be set in AETrainer constructor.\")\n",
        "\n",
        "        # reshape para 3D (n_windows, T, S)\n",
        "        X_train = X_train_flat.reshape(-1, self.window_length, self.n_sensors)\n",
        "\n",
        "        self.model = self._build_model(self.window_length, self.n_sensors)\n",
        "\n",
        "        es = callbacks.EarlyStopping(monitor='val_loss', patience=self.patience, restore_best_weights=True, verbose=0)\n",
        "        history = self.model.fit(\n",
        "            X_train, X_train,\n",
        "            epochs=self.epochs,\n",
        "            batch_size=self.batch_size,\n",
        "            validation_split=0.1,\n",
        "            callbacks=[es],\n",
        "            verbose=self.verbose\n",
        "        )\n",
        "\n",
        "        # compute reconstruction errors on train (to set threshold)\n",
        "        X_recon = self.model.predict(X_train, batch_size=self.batch_size, verbose=0)\n",
        "        recon_err = np.mean((X_train - X_recon) ** 2, axis=(1,2))\n",
        "        self.train_recon_err_ = recon_err\n",
        "        self.threshold = float(np.percentile(recon_err, self.threshold_percentile))\n",
        "        self._train_shape = X_train.shape\n",
        "        print(\"Training complete\")\n",
        "\n",
        "    def anomaly_score(self, X_flat: np.ndarray) -> np.ndarray:\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not fitted.\")\n",
        "        X = X_flat.reshape(-1, self.window_length, self.n_sensors)\n",
        "        X_recon = self.model.predict(X, batch_size=self.batch_size, verbose=0)\n",
        "        recon_err = np.mean((X - X_recon) ** 2, axis=(1,2))  # greater => more anomalous\n",
        "        return recon_err\n",
        "\n",
        "    def TrainerPred(self, X_flat: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Return 1 = normal, 0 = anomaly (keeps your convention)\"\"\"\n",
        "        if self.threshold is None:\n",
        "            raise ValueError(\"Model not fitted (threshold missing).\")\n",
        "        scores = self.anomaly_score(X_flat)\n",
        "        preds = (scores <= self.threshold).astype(int)\n",
        "        return preds\n",
        "\n",
        "\n",
        "WINDOW_SIZE_DL = 60\n",
        "WINDOW_OVERLAP_DL = 15\n",
        "n_features_per_timestep = pp.normal.shape[1]\n",
        "\n",
        "window_hyperparameters = {\n",
        "    \"new_window_size\": [WINDOW_SIZE_DL],\n",
        "    \"window_overlap\": [WINDOW_OVERLAP_DL],\n",
        "    \"dimensionsPerSample\": [n_features_per_timestep]\n",
        "}\n",
        "\n",
        "model_hyperparameters = {\n",
        "    \"filters\": [[8, 6], [12, 8]],           # versão leve e versão mais capaz\n",
        "    \"kernel_sizes\": [[3,3], [5,3]],\n",
        "    \"latent_channels\": [4, 8],\n",
        "    \"batch_size\": [64],\n",
        "    \"epochs\": [60],\n",
        "    \"lr\": [1e-3],\n",
        "    \"patience\": [5],\n",
        "    \"threshold_percentile\": [97.5, 99],\n",
        "    # sttatic\n",
        "    \"window_length\": [WINDOW_SIZE_DL],\n",
        "    \"n_sensors\": [n_features_per_timestep],\n",
        "    \"random_seed\": [42],\n",
        "    \"verbose\": [0]\n",
        "}\n",
        "\n",
        "evaluator = Evaluator()\n",
        "\n",
        "tuner = DeepLearningTuner(\n",
        "    window_params=window_hyperparameters,\n",
        "    modelTrainerParams=model_hyperparameters,\n",
        "    X_train=X_train,    # raw or flattened windows acceptable: DeepLearningTuner will call resizeFlattenedWindow\n",
        "    X_val=X_val,\n",
        "    X_test=X_test,\n",
        "    anom_val=anom_val,\n",
        "    anom_test=anom_test\n",
        ")\n",
        "\n",
        "tuner.tune(Trainer_factory=AETrainer, evaluator=evaluator)\n",
        "\n",
        "if tuner.best_params is not None:\n",
        "    print(\"\\nBest configuration:\")\n",
        "    print(\"Window Params:\", tuner.best_params[\"window_params\"])\n",
        "    print(\"Model Params:\", tuner.best_params[\"model_params\"])\n",
        "    print(f\"Best validation score: {tuner.best_score:.4f}\")\n",
        "    best_ae = tuner.best_model\n",
        "else:\n",
        "    print(\"Nenhum modelo válido encontrado.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "zN26TgAqfLRp",
        "outputId": "cf42ec57-7fce-4950-de82-ebd7bd51a85d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Window Config: {'new_window_size': 60, 'window_overlap': 15, 'dimensionsPerSample': 10}\n",
            "Training Autoencoder\n",
            "Training complete\n",
            "{'filters': [8, 6], 'kernel_sizes': [3, 3], 'latent_channels': 4, 'batch_size': 64, 'epochs': 60, 'lr': 0.001, 'patience': 5, 'threshold_percentile': 97.5, 'window_length': 60, 'n_sensors': 10, 'random_seed': 42, 'verbose': 0}\n",
            "Training Autoencoder\n",
            "Training complete\n",
            "{'filters': [8, 6], 'kernel_sizes': [3, 3], 'latent_channels': 4, 'batch_size': 64, 'epochs': 60, 'lr': 0.001, 'patience': 5, 'threshold_percentile': 99, 'window_length': 60, 'n_sensors': 10, 'random_seed': 42, 'verbose': 0}\n",
            "Training Autoencoder\n",
            "Training complete\n",
            "Training Autoencoder\n",
            "Training complete\n",
            "Training Autoencoder\n",
            "Training complete\n",
            "Training Autoencoder\n",
            "Training complete\n",
            "{'filters': [8, 6], 'kernel_sizes': [5, 3], 'latent_channels': 4, 'batch_size': 64, 'epochs': 60, 'lr': 0.001, 'patience': 5, 'threshold_percentile': 99, 'window_length': 60, 'n_sensors': 10, 'random_seed': 42, 'verbose': 0}\n",
            "Training Autoencoder\n",
            "Training complete\n",
            "Training Autoencoder\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3686895829.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAETrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3801364082.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(self, Trainer_factory, evaluator)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainerFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3686895829.py\u001b[0m in \u001b[0;36mTrainerFit\u001b[0;34m(self, X_train_flat, Y_train)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         history = self.model.fit(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.best_params"
      ],
      "metadata": {
        "id": "QNy1ljNiQVXr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}